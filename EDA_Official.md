---
title: "Home Credit EDA"
format: 
  html:
    theme: flatly
    df_print: paged
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
execute:
  include: true
  eval: true    
  warning: false
  message: false
self-contained: true
---

# Business Problem Statement

The problem is that Home Credit currently serves clients who cannot be served by the traditional banking system or other traditional financial institutions. These clients cannot be served by traditional institutions because they are usually from underprivileged populations. This results in them having insufficient/non-existent credit history. It also makes it difficult for Home Credit to utilize traditional measures like FICO Scores to see if a client can repay their loan.

If Home Credit approves a loan for a client and they cannot pay the loan back, then it is a financial loss for Home Credit. However, if a client can pay back a loan but is denied, it represents a loss of potential revenue for Home Credit. Both scenarios ultimately affect Home Creditâ€™s ability to operate efficiently because Home Credit either loses money from bad loans or forgoes lending opportunities.

# Load Libraries

```{r}
# load libraries
pacman::p_load(tidyverse,skimr,janitor,knitr,caret,rminer,mice,dbscan)

```

# Read in Datasets

```{r}
# read in datasets
train_set <- read_csv("C:\\Users\\User\\Box Sync\\Business Analytics Degree\\Semesters\\Fall Semester 2024\\IS 6812\\Data\\home-credit-default-risk\\application_train.csv")

```

There are 7 extra files in addition to the train_set that can be utilized for prediction. They are all connected primarily via the SK_ID_CURR variable. The EDA will primarily focus on the train_set. It should be noted that other data sets will be incorporated to help improve the models if needed.

# View Data

```{r}
head(train_set) # get first 6 rows of dataset
```

# Exploratory Questions

There are several variables in this dataset that have interesting relationships. The following contains a list of questions: 

- Is there a relationship between a client's contract type and default status? 

- What is the relationship between a client's occupation and default status? 

- What is the relationship between a client's education and default status? 

- What is the relationship between a client's marital status and default status? 

- What is the relationship between a client's income type and default status? 

- Is there a relationship between the credit scores and a client's default status? 

- What is the relationship between all the annuity amount, annuity credit, the cost of the item that has caused them to apply for a loan, etc. 

- Is there a relationship between gender and a client's default status? 

- Does the age of a client's car have any contribution, since cars are typically used for collateral in loans? 

- Does the age of the client have any impact on their ability to get loans, credit etc? Could it impact their ability to purchase a car which could potentially be used as collateral? Consequently would this affect their ability to repay a loan?

# Class Prevalence

```{r}

table(train_set$TARGET) # Get a count of the target, default and no default.
round(prop.table(table(train_set$TARGET)),4) * 100 # Multiply by 100 to get percentages

```

Clients who have difficulties paying back loans are represented as a 1 in the dataset while 0 represents all the other cases. (0 presumably means that the client was able to pay back their loan but may have had some case not related to paying back loans.) The clients who paid back their loans is more prevalent at 91.93% while the clients who had payment difficulties appear at 8.07%. This means that there is a large class imbalance.

It should be noted that the objective is to predict clients who can pay back their loans. Hence, since 91.93% of the clients paid back their loan, the model has a lot of information to learn about successful repayment.

# Build Majority Classifier

```{r}

# Define the counts
yes_count <- 282686
no_count <- 24825

# Calculate the total number of observations
total_count <- yes_count + no_count

# Majority class
majority_class <- ifelse(yes_count > no_count, 1, 0) # 1 if yes_count is greater than no count

# Convert Majority Class to factor
majority_class <- factor(majority_class)

#Generate the majority classifier predictions for all instances
predicted <- rep(majority_class, total_count) # Total count represents the total number of instances
# since majority class is zero, this will be repeated for every prediction or total number of instances.

# Create metrics list for Majority Classifier  
metrics_list = c("ACC","TPR","PRECISION", "F1", "CONF")

# Generate metrics
majority_output <- mmetric(factor(train_set$TARGET), predicted, metrics_list) #Set all the metrics generated by the mmetric function to an output.

majority_output_rounded <- lapply(majority_output, function(x) { # Utilize function to round off metrics to 2 decimal places.
  if (is.numeric(x)) {
    return(round(x, 2))
  } else {
    return(x)
  }
})
print(majority_output_rounded)

```

TPR stands for Recall

The majority classifier has an overall accuracy of 91.93%. This is however because the majority class (0 - pay back loan) comprises the dataset majority. (Majority Classifiers predict the majority class for everything which means that accuracy will be the majority class value.)

However since the objective is to predict the clients who can pay back their loans, the majority class accuracy can be used as the baseline accuracy. This also means that any future model will have to beat this majority class accuracy. (91.93%)

Additional metrics that may be desirable to be beat is `Precision 1` and `F1-1` . It should be noted that beating the recall for 1(pay back loan) will not be possible since it is 100%. This is because TPR1 represents the majority class and the majority classifier will make no false negative errors. Thus, everything will be classified as positive which divided by the total positive observations results in 100.

- True Positive represents correctly predicting that somebody will be able to pack a loan.
- True Negative represents correctly predicting that somebody will have difficulty paying back a loan.
- False Negative represents incorrectly predicting difficulty paying back the loan.
- False Positive represents incorrectly predicting that somebody will pay back the loan when they actually will have difficulty paying back the loan.

# Build Random Classifier

```{r}

class_labels = c(0,1) # Create a class label with 2 labels, 0 and 1
class_labels <- factor(class_labels) # Convert Class labels to a factor

set.seed(123) # Set seed for reproducibility
predictions <- sample(x = class_labels, size = total_count,replace = TRUE, prob = c(.5,.5)) # Sample from the class label with replacement. Each observation has a 50% chance of being selected.

predictions <- factor(predictions) # Convert the predictions into a factor

random_output <- mmetric(factor(train_set$TARGET),predictions,metrics_list) # Generate metrics


random_output_rounded <- lapply(random_output, function(x) { # Utilize function to round off metrics to 2 decimal places.
  if (is.numeric(x)) {
    return(round(x, 0))
  } else {
    return(x)
  }
})
print(random_output_rounded) # Print out the rounded metrics

```

The random classifier model has an overall accuracy of 50% due to randomly assigning an observation to either default or no default with 50% probability. Consequently, this has also affected recall which measures the following:

predicted class/total observations in the actual class

The objective of any model built will be to have an accuracy higher than 50% and to have a higher recall (greater than TPR1), higher precision1, and a higher F11 score as well.

# Check Low Variance Columns

## Create two datasets

```{r}

train_clean <- train_set # Assign train_set to train_clean set
train_set_02 <- train_set # Assign train_set to train_set_02

```

train_clean will be used to officially implement any changes, data cleaning etc.

train_set_02 will be used to experiment on the dataset and try changes out before officially implementing it in the train_clean.

Additionally, it is important to retain the original dataset as a backup.

## Define Function to Check for NA's

```{r}

# Define function to check for NAs
find_na <- function(x) sum(is.na(x))

# Apply function to each column with map()
missing_values <- map(.x = train_set, .f = find_na) %>% # Assign changes to a new variable
  unlist() %>% 
  data.frame()

missing_values <- missing_values %>% # Overwrite the missing_values with a new dataframe
  rownames_to_column(var = "Column") %>% 
  rename(Missing_Values = 1) # Rename the unnamed column to "Missing_Values"

```

This code will create a dataframe that displays every variable and how many missing values per variable.

## Cross Reference Low Variance Columns with no Missing Values

The missing values will be cross referenced with the Low Variance columns in order to see which Low Variance Columns have no missing values.

```{r}

# Identify near zero variance predictors by name
near_zero_variance <- nearZeroVar(train_set, names = TRUE)


#Check the near zero variance columns that have no missing values
cols_with_zero_missing <- missing_values[missing_values$Missing_Values %in% near_zero_variance & missing_values$. == 0, ]

# Calculate the variances for these columns
zero_missing_cols <- cols_with_zero_missing$Missing_Values
variances_zero_missing <- sapply(zero_missing_cols, function(col) var(train_set[[col]], na.rm = TRUE))

#Print the results
print(variances_zero_missing)

```

The variances for these columns are very small and incredibly close to zero. It is very unlikely that these predictors will contain any relevant information for prediction. It additionally may cause issues with cross validation, and slow down prediction algorithms since the dataset's dimensionality is large.

- Additionally, for the `Flag_Document_Columns`, it is unclear what Document 1 to 20 represents. The full context is unknown, which makes it difficult to glean any useful information from these columns. Thus, any `Flag_Document_Columns` that has low variance can be dropped.

- Moreover, for the `REG_REGION_NOT_LIVE_REGION` and `LIVE_REGION_NOT_WORK_REGION`, there are similar predictors that measure similar characteristics like `REG_REGION_NOT_WORK_REGION`, `REG_CITY_NOT_LIVE_CITY`, etc. Hence, these two columns can be dropped as well.

The `Days Employed` column however requires further exploration. This is because `Days Employed` has a very high variance at 19,958,840,000. Thus, further investigation on this discrepancy is required.

### Plot Histogram for Days Employed

```{r}

# Create a histogram to visualize the distribution
ggplot(train_set, aes(x = DAYS_EMPLOYED)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(title = "Distribution of Days Employed", x = "Days Employed", y = "Count")

```

The histogram above shows that Days Employed is a bimodal distribution with one large peak and a smaller peak around 0 days. It also has a count that is greater than 250,000 when both peaks are combined. The dataset has 307,511 rows which means that 0 days comprises roughly 81.3% of the rows in the dataset.

This would also explain why the Near Zero Variance Function considered this to be low variance since most of the values were clustered around zero relative to the rest of the data distribution. There are a few things to note however:

- The high number of observations around zero seems unrealistic in the context of the Days Employed variable. The "Column Description" file states that this variable represents how many days a client started employment before the application. It seems very unlikely that 81% of the applicants started their employment and got a loan on the same day. The 0 days thus seems to be some sort of rubber-stamp procedure or some recording policy Home Credit has created.

- One could argue that Home Credit serves people who can't qualify for traditional loans. Thus, there will be some unusual discrepancies/features when compared to a more traditional loan approval dataset. Although this may be true, it still seems very unlikely that clients would get a job and apply for a loan on the same day. Other predictors like credit card scores might be different for this dataset, however this predictor does not seem like that.

The other peak occurs around 3.5e+05 which represents 350,000 days or 958 years.

- This is a very unrealistic value.

Since the values for this histogram are very unrealistic, this predictor will be dropped from the dataset.

### Drop near zero variance predictors (no missing values)

```{r}
train_clean <- train_clean %>% #Drop all the selected predictors
  select(-DAYS_EMPLOYED,-FLAG_MOBIL, -FLAG_CONT_MOBILE,  -REG_REGION_NOT_LIVE_REGION, -LIVE_REGION_NOT_WORK_REGION, -FLAG_DOCUMENT_2, -FLAG_DOCUMENT_4, -FLAG_DOCUMENT_5, -FLAG_DOCUMENT_7, -FLAG_DOCUMENT_9, -FLAG_DOCUMENT_10,       -FLAG_DOCUMENT_11, -FLAG_DOCUMENT_12, -FLAG_DOCUMENT_13, -FLAG_DOCUMENT_14, -FLAG_DOCUMENT_15, -FLAG_DOCUMENT_16,          -FLAG_DOCUMENT_17, -FLAG_DOCUMENT_18, -FLAG_DOCUMENT_19, 
- FLAG_DOCUMENT_20, -FLAG_DOCUMENT_21)
```

These columns are the low variance columns that have no missing values which will be dropped from the `train_clean` dataset.

## Cross Reference Low Variance Columns with Missing Values

The rest of the low variance columns will be cross referenced with the missing values to see which columns have both low variance and missing values.

```{r}

# Check the near zero variance columns that have missing values
cols_with_missing <- missing_values[missing_values$Missing_Values %in% near_zero_variance & missing_values$. > 0, ]


# Calculate the variances for these columns
missing_cols <- cols_with_missing$Missing_Values
variances_missing <- sapply(missing_cols, function(col) var(train_set[[col]], na.rm = TRUE))


missing_count <- sapply(missing_cols, function(col) sum(is.na(train_set[[col]])))

# Combine the variances and missing counts into a data frame
results_with_missing <- data.frame(
  Variable = names(variances_missing),
  Variance = variances_missing,
  Missing_Count = missing_count
)

# Print the results
print(results_with_missing)

```

The following variables all describe different aspectes of the client's home which has been normalized per the Home Credit Columns Description File:

- `BASEMENTAREA_AVG`
- `LANDAREA_AVG`
- `NONLIVINGAREA_AVG`
- `BASEMENTAREA_MODE`
- `LANDAREA_MODE`
- `NONLIVINGAREA_MODE`
- `BASEMENTAREA_MEDI`
- `LANDAREA_MEDI`
- `NONLIVINGAREA_MEDI`

The next following set of variables describe similar metrics per the Home Credit Columns Description File:

- `AMT_REQ_CREDIT_BUREAU_HOUR`
- `AMT_REQ_CREDIT_BUREAU_DAY`
- `AMT_REQ_CREDIT_BUREAU_WEEK`

The last following set of variables that are similar and have low variance are:

- `HOUSETYPE_MODE`
- `EMERGENCYSTATE_MODE`

Thus, to increase the variance, some of these predictors will be combined into one predictor via Principal Component Analysis.

### Build Housing Related Correlation Matrix for PCA

```{r}
# Define the predictors to check for correlation
predictors_to_combine <- c("BASEMENTAREA_AVG", "LANDAREA_AVG", "NONLIVINGAREA_AVG","NONLIVINGAREA_AVG","BASEMENTAREA_MODE","LANDAREA_MODE","NONLIVINGAREA_MODE","BASEMENTAREA_MEDI","LANDAREA_MEDI","NONLIVINGAREA_MEDI")

# Calculate the correlation matrix (ignoring missing values)
correlation_matrix <- cor(train_set_02[predictors_to_combine], use = "complete.obs")

# Display the correlation matrix
correlation_matrix
```

Some variables are highly correlated with each other like `LANDAREA_MEDI` and `LANDAREA_AVG` at .99. Other variables however are somewhat correlated like `BASEMENTAREA_MEDI` and `LANDAREA_MEDI` at 0.46.

This indicates that Principal Component Analysis should be fairly appropriate for combining these columns into one predictor.

### Combine Housing Related Predictors

```{r}

for (col in predictors_to_combine) { # For-Loop for all the columns in the pred_to_combine vector
  train_clean[[col]] <- ifelse(is.na(train_clean[[col]]), # Get all the columns from pred_to_combine with missing values 
  median(train_clean[[col]], na.rm = TRUE),train_clean[[col]]) # Impute those values with the median, ignore NA values
} 

pca_result <- prcomp(train_clean[predictors_to_combine], na.action = na.omit, scale. = TRUE) # Assign results of PCA to a column

summary(pca_result) # Get summary of PCA

# Extract the first principal component
pca_scores <- pca_result$x[, 1]  # First principal component

train_clean$House_Attribute_Low_Variance <- pca_scores #Create a new col and assign pca scores to that column
var(train_clean$House_Attribute_Low_Variance) # Show the variance of the house_attribute column

```

The Housing Related Predictors with low columns have first had any missing values imputed with the median. Afterwards, the predictors have been combined into one column using PCA. 

### Remove all the housing related low variance predictors

```{r}

train_clean <- train_clean %>% #Remove the following predictors
  select(-BASEMENTAREA_AVG, -LANDAREA_AVG, -NONLIVINGAREA_AVG,-NONLIVINGAREA_AVG,-BASEMENTAREA_MODE,-LANDAREA_MODE,-NONLIVINGAREA_MODE,-BASEMENTAREA_MEDI,-LANDAREA_MEDI,-NONLIVINGAREA_MEDI)

```

It should be noted that all the housing columns that have low variance have been combined into one column, which means that the rest of the housing columns can be removed.

### Build Bureau Correlation Matrix for PCA

```{r}
# b = bureau
# Define the predictors you want to check for correlation
b_predictors_to_combine <- c("AMT_REQ_CREDIT_BUREAU_HOUR",	
"AMT_REQ_CREDIT_BUREAU_DAY", "AMT_REQ_CREDIT_BUREAU_WEEK")

# Calculate the correlation matrix (ignoring missing values)
b_correlation_matrix <- cor(train_set_02[b_predictors_to_combine], use = "complete.obs")

#Display correlation matrix
b_correlation_matrix

```

The correlation is pretty weak, nothing very strong, which means that PCA would be inappropriate. The counts for the variable however should still be analyzed.

### Count of Bureau Required Variables

```{r}

# Get a count of how many inquiries
table(train_set$AMT_REQ_CREDIT_BUREAU_HOUR)
table(train_set$AMT_REQ_CREDIT_BUREAU_DAY)
table(train_set$AMT_REQ_CREDIT_BUREAU_MON)

```

The number of inquires seems relatively limited with month having the highest amount of inquires at 27 for one observation.

Although these variables have low variance, they do seem like they could still be useful for predicting whether someone will pay back a loan or not.

However, NA values must be first addressed.

### Address Missing Bureau Required Values

```{r}

# AMT_REQ_CREDIT_BUREAU_HOUR
train_clean$AMT_REQ_CREDIT_BUREAU_HOUR[is.na(train_set$AMT_REQ_CREDIT_BUREAU_HOUR)] <- 0
sum(is.na(train_clean$AMT_REQ_CREDIT_BUREAU_HOUR))

# AMT_REQ_CREDIT_BUREAU_DAY
train_clean$AMT_REQ_CREDIT_BUREAU_DAY[is.na(train_set$AMT_REQ_CREDIT_BUREAU_DAY)] <- 0
sum(is.na(train_clean$AMT_REQ_CREDIT_BUREAU_DAY))

# AMT_REQ_CREDIT_BUREAU_MON
train_clean$AMT_REQ_CREDIT_BUREAU_MON[is.na(train_set$AMT_REQ_CREDIT_BUREAU_MON)] <- 0
sum(is.na(train_clean$AMT_REQ_CREDIT_BUREAU_MON))

```

The NA values have two possible meanings: 

* There is genuinely no information on the inquiries made. This seems unlikely that Home Credit would forget to record how many inquires they make about clients. 

* The NA values could be another way of representing zero inquiries or no information which could imply zero inquiries. Thus the NA values will be imputed as zero.

### Check Counts of HOUSETYPE_MODE and EMERGENCYSTATE_MODE

```{r}
# Get a count of the values
table(train_set_02$HOUSETYPE_MODE)

# Get a count of the values
table(train_set_02$EMERGENCYSTATE_MODE)

```

These seem like categorical variables that have distinct levels. The missing values therefore can be addressed with imputation.

### Impute Missing Values for HOUSETYPE_MODE

```{r}
# Define a function in R that finds the mode
find_mode <- function(x){
  table(x) %>% 
    which.max() %>% #Find which category appears the most
    names() #Get the name of this category
}

housetype_mode <- find_mode(train_clean$HOUSETYPE_MODE) #Use this function to find mode and assign value to houstetype_mode


train_clean$HOUSETYPE_MODE[is.na(train_clean$HOUSETYPE_MODE)] <- housetype_mode #Impute the train_clean dataset with the mode
```

The missing values for `HOUSETYPE_MODE` have been imputed with the mode.

### Convert HOUSETYPE_MODE to factor

```{r}
#Convert HouseType_Mode to a factor
train_clean$HOUSETYPE_MODE <- as.factor(train_clean$HOUSETYPE_MODE)

```

The `Housetype Mode` column also consists of categorical values which have no meaningful numeric difference. Thus, this variable will be converted into a factor.

### Impute Missing Values for EMERGENCYSTATE_MODE

```{r}

# Get a count of the values
table(train_set_02$EMERGENCYSTATE_MODE)
# See how many missing values are in the columns
sum(is.na(train_set_02$EMERGENCYSTATE_MODE))

# Set variable with string "Unknown"
unknown <- "Unknown"

# Replace all NA values with this "Unkown" value
train_clean$EMERGENCYSTATE_MODE[is.na(train_clean$EMERGENCYSTATE_MODE)] <- unknown

# Convert Variable to a factor
train_clean$EMERGENCYSTATE_MODE <- as.factor(train_clean$EMERGENCYSTATE_MODE)

```

As shown previously, there are only 2 values Emergency State Mode. (Emergency State Mode seems to describe the state of the client's residence)

There are however missing values that must be addressed. The missing values could be due to the client not providing the information or Home Credit not insisting on the information, etc.

This however does not necessarily mean that the information is missing. Instead, the information could be unknown which in itself could be a helpful predictor. Thus, the `NA` values will be imputed with "unknown" which will also enable various algorithms to utilize the "unknown" when making predictions.

# Check Missing Variables

## How many missing columns

```{r}
#tc = train_clean
missing_values_tc <- map(.x = train_clean, .f = find_na) %>% 
  unlist() %>% 
  data.frame() #Assign the df w/ all missing values to train_clean

missing_values_tc <- missing_values_tc %>% #override missing_values_tc with new changes
  rownames_to_column(var = "Column") %>% 
  rename(Missing_Values = 1) # Rename the unnamed column to "Missing_Values"

missing_values_tc %>% 
  filter( . != 0) %>% #Show only missing columns
  summarise(missing_col_cnt  = length(.), # compute missing values as percentages
            missing_col_pctg = (length(.)/122) * 100) %>% round(2)

```

Since the low variance columns and its missing values have been addressed, the rest of the dataset will be examined for any missing values. The train_clean dataset will be utilized since some of the low variance columns were dropped. Thus utilizing this dataset will give us a more accurate count.

Overall, there are 53 columns with missing values which comprise 43.44% of this dataset.

## Names of missing columns

```{r}
missing_values_tc %>% 
  filter( . != 0) %>% # Show only non-zero columns
  arrange(desc(.)) # Arrange in descending order 
```

These are the actual names of the columns that are missing values. Interestingly several of them seem to be related and have the same missing values. For instance:

- `NONLIVINGAPARTMENTS_AVG`
- `NONLIVINGAPARTMENTS_MODE`
- `NONLIVINGAPARTMENTS_MEDI`

These 3 variables are all missing 213,514 observations.

## Columns greater than 50%

```{r}
missing_values_tc %>% 
  filter( . != 0) %>% #Show only non-zero columns
  mutate(col_pctg_missing_values = ./nrow(train_clean)) %>% # Get decimal by dividing by # of rows in df
  filter(col_pctg_missing_values >=.50) %>% # Filter values that are greater than 50%
  select(Missing_Values,col_pctg_missing_values) %>% # Select only missing values and the pctg.
  arrange(desc(col_pctg_missing_values)) # Arrange in descending order
```

These are all the variables that have 50% or more of the values missing. It should be noted that the vast majority of these variables are all related. The variables are summary statistics describing various information about a client's residence.

For instance:

- `COMMONAREA_AVG`
- `COMMONAREA_MODE`
- `COMMONAREA_MEDI`

These three columns are describing the average size of a common area, the mode of the common area (how many common areas are in a client's residence) and the median size of a common area.

The other variables are also describing similar metrics for other parts of the client's residence like the living room, etc.

### Create a Correlation Matrix for Housing Attribute Variables

```{r}

# Extract relevant columns for COMMONAREA
commonarea_cols <- train_clean %>% 
  select(contains("COMMONAREA"))
# Run correlation for COMMONAREA columns
cor_commonarea <- cor(commonarea_cols, use = "complete.obs")

# Extract relevant columns for NONLIVINGAPARTMENTS
nonliving_cols <- train_clean %>% 
  select(contains("NONLIVINGAPARTMENTS"))
# Run correlation for NONLIVINGAPARTMENTS columns
cor_nonliving <- cor(nonliving_cols, use = "complete.obs")

# Select only the columns related to 'LIVINGAPARTMENTS'
living_apartments_cols <- train_clean %>% 
  select(starts_with("LIVINGAPARTMENTS"))
# Calculate the correlation matrix for these columns
cor_living <- cor(living_apartments_cols, use = "complete.obs")

# Select only the columns related to 'FLOORSMIN'
floorsmin_col <- train_clean %>%
  select(starts_with("FLOORSMIN"))
# Calculate the correlation matrix for these columns
cor_floorsmin <- cor(floorsmin_col, use = "complete.obs")

# Select only the columns related to 'YEARS_BUILD'
yearsbuild_col <- train_clean %>%
  select(starts_with("YEARS_BUILD"))
# Calculate the correlation matrix for these columns
cor_yearsbuild <- cor(yearsbuild_col, use = "complete.obs")

# Select only the columns related to 'ELEVATORS'
elevators_col <- train_clean %>%
  select(starts_with("ELEVATORS"))
# Calculate the correlation matrix for these columns
cor_elevators <- cor(elevators_col, use = "complete.obs")

# Select only the columns related to 'APARTMENTS'
apartments_col <- train_clean %>%
  select(starts_with("APARTMENTS"))
# Calculate the correlation matrix for these columns
cor_apartments <- cor(apartments_col, use = "complete.obs")

# Select only the columns related to 'ENTRANCES'
entrances_col <- train_clean %>%
  select(starts_with("ENTRANCES"))
# Calculate the correlation matrix for these columns
cor_entrances <- cor(entrances_col, use = "complete.obs")

# Select only the columns related to 'ENTRANCES'
Living_area_col <- train_clean %>%
  select(starts_with("LIVINGAREA"))
# Calculate the correlation matrix for these columns
cor_Living_Area <- cor(Living_area_col, use = "complete.obs")

# Print the results
print("Correlation for COMMONAREA variables:")
print(cor_commonarea)

print("Correlation for NONLIVINGAPARTMENTS variables:")
print(cor_nonliving)

print("Correlation for LIVINGAPARTMENTS variables:")
print(cor_living)

print("Correlation for FLOORSMIN variables:")
print(cor_floorsmin)

print("Correlation for YEARS_BUILD variables:")
print(cor_yearsbuild)

print("Correlation for ELEVATORS variables:")
print(cor_elevators)

print("Correlation for APARTMENTS variables:")
print(cor_apartments)

print("Correlation for ENTRANCES variables:")
print(cor_entrances)

print("Correlation for LIVINGAREA variables:")
print(cor_Living_Area)

```

The following variables are all very highly correlated with each other and are missing approximately 70% of their column values.

Additionally, only one of the variables is required since the variables are measuring different aspects of the same attribute as previously mentioned (i.e median of common area, mean of common area, etc.) Thus, only of one of the columns is required which will be the median. The median is a more reliable estimate compared to the other 2 metrics.

Moreover for the mode, it's very unlikely that someone will have a multiple common areas, living areas, etc. Thus, the mode can also be removed and it's also highly correlated with the median columns which means that very little information will be lost. It'll also help with creating a more parsimonious model and reducing noise.

### Impute Missing Values with MICE

The mice package will be utilized to impute the missing values

```{r}
# Create a vector of columns to be imputed
columns_to_impute <- c("COMMONAREA_MEDI", "LIVINGAPARTMENTS_MEDI",	
"NONLIVINGAPARTMENTS_MEDI",
"FLOORSMIN_MEDI",	
"YEARS_BUILD_MEDI",
"ELEVATORS_MEDI",	
"APARTMENTS_MEDI"	,
"ENTRANCES_MEDI",
"LIVINGAREA_MEDI")

# Create variables that contain similar columns
common_area <- c('COMMONAREA_AVG','COMMONAREA_MODE','COMMONAREA_MEDI') 
living_apartments <- c("LIVINGAPARTMENTS_AVG", "LIVINGAPARTMENTS_MODE", "LIVINGAPARTMENTS_MEDI")
non_living_apartments <- c("NONLIVINGAPARTMENTS_AVG", "NONLIVINGAPARTMENTS_MODE",                  "NONLIVINGAPARTMENTS_MEDI")
floorsmin <- c("FLOORSMIN_AVG", "FLOORSMIN_MODE", "FLOORSMIN_MEDI")
years_build <- c("YEARS_BUILD_AVG", "YEARS_BUILD_MODE", "YEARS_BUILD_MEDI")
elevators <- c("ELEVATORS_AVG", "ELEVATORS_MODE", "ELEVATORS_MEDI")
apartments <- c("APARTMENTS_AVG" ,"APARTMENTS_MODE", "APARTMENTS_MEDI")
entrances <- c("ENTRANCES_AVG" ,"ENTRANCES_MODE", "ENTRANCES_MEDI")
living_area <- c("LIVINGAREA_AVG", "LIVINGAREA_MODE", "LIVINGAREA_MEDI")


# Create a dataset with only the relevant columns
common_area_subset <- train_clean[, common_area]
# Use mice to impute missing values
common_area_subset_imputed <- mice(common_area_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_common_area_subset <- complete(common_area_subset_imputed)


# Create a dataset with only the relevant columns
living_apartments_subset <- train_clean[, living_apartments]
# Use mice to impute missing values
living_apartments_imputed <- mice(living_apartments_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_living_apartments_subset <- complete(living_apartments_imputed)


# Create a dataset with only the relevant columns
non_living_apartments_subset <- train_clean[, non_living_apartments]
# Use mice to impute missing values
non_living_apartments_imputed <- mice(non_living_apartments_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_non_living_apartments <- complete(non_living_apartments_imputed)


# Create a dataset with only the relevant columns
floorsmin_subset <- train_clean[, floorsmin]
# Use mice to impute missing values
floorsmin_subset_imputed <- mice(floorsmin_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_floorsmin_subset <- complete(floorsmin_subset_imputed)


# Create a dataset with only the relevant columns
years_build_subset <- train_clean[, years_build]
# Use mice to impute missing values
years_build_imputed <- mice(years_build_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_years_build_subset <- complete(years_build_imputed)


# Create a dataset with only the relevant columns
elevators_subset <- train_clean[, elevators]
# Use mice to impute missing values
elevators_imputed <- mice(elevators_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_elevators_subset <- complete(elevators_imputed)


# Create a dataset with only the relevant columns
apartments_subset <- train_clean[, apartments]
# Use mice to impute missing values
apartments_imputed <- mice(apartments_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_apartments_subset <- complete(apartments_imputed)


# Create a dataset with only the relevant columns
entrances_subset <- train_clean[, entrances]
# Use mice to impute missing values
entrances_imputed <- mice(entrances_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_entrances_subset <- complete(entrances_imputed)


# Create a dataset with only the relevant columns
living_area_subset <- train_clean[, living_area]
# Use mice to impute missing values
living_area_imputed <- mice(living_area_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)
# Get the completed dataset
finished_data_living_area_subset <- complete(living_area_imputed)

# Replace the column with the imputed column values from the MICE Package
train_clean$COMMONAREA_MEDI <- finished_data_common_area_subset$COMMONAREA_MEDI
train_clean$LIVINGAPARTMENTS_MEDI <- finished_data_living_apartments_subset$LIVINGAPARTMENTS_MEDI
train_clean$NONLIVINGAPARTMENTS_MEDI <- finished_data_non_living_apartments$NONLIVINGAPARTMENTS_MEDI
train_clean$FLOORSMIN_MEDI <- finished_data_floorsmin_subset$FLOORSMIN_MEDI
train_clean$YEARS_BUILD_MEDI <- finished_data_years_build_subset$YEARS_BUILD_MEDI
train_clean$ELEVATORS_MEDI <- finished_data_elevators_subset$ELEVATORS_MEDI
train_clean$APARTMENTS_MEDI <- finished_data_apartments_subset$APARTMENTS_MEDI
train_clean$ENTRANCES_MEDI <- finished_data_entrances_subset$ENTRANCES_MEDI
train_clean$LIVINGAREA_MEDI <- finished_data_living_area_subset$LIVINGAREA_MEDI

```

The MICE function from the MICE library has been implemented to combine information from the correlated predictors and impute the missing values for the median columns.

It does this by iteratively modelling the column values by using the selected variables as predictors. For instance, to impute the median of the "Common Area" variable, the MICE package will utilize the `COMMONAREA_AVG` and `COMMONAREA_MODE` variables to impute missing values for the `COMMONAREA_MEDI`. It will do this by creating a predictive model utilizing the `COMMONAREA_AVG` and `COMMONAREA_MODE` predictors to make predictions about the missing values.

Since these columns are highly correlated, the MICE algorithm will be able to utilize these values to come up with plausible values for the `COMMONAREA_MEDI` column.

The method utilized is "pmm" which is Predictive Mean Matching. (PMM) This is utilized for continuous data to ensure that imputed values are plausible. (This is done by selecting an actual variable from a similar case which would be `COMMONAREA_AVG` and `COMMONAREA_MODE` for this scenario.)

This process is then repeated for all of the other variables. This method is more robust then imputation with the median because it can take the other columns into account and their effect on the column being imputed.

### Drop the Non-Median Columns

```{r}

train_clean <- train_clean %>% 
  select(-ends_with("_AVG")) %>% # Drop any column that ends with "AVG"
  select(-ends_with("_MODE")) # Drop any column that ends with "MODE"

```

The missing values in the `MEDI` columns have been successfully imputed. This means that any column that ended with `AVG` or `MODE` which were our `APARTMENTS_AVG`, `ELEVATORS_AVG`, `ELEVATORS_MODE`, etc. should be dropped. (The above code does this.)

It should be noted that `FONDKAPREMONT_MODE` and `WALLSMATERIAL_MODE` were also dropped as a result. These variables however had no other associated columns and were missing 68.38% and 50.84% values respectively. A lot of information will not be lost if these columns are dropped from the dataset. It'll also help make any future models more parsimonious.

### Remaining Missing Values Greater Than 50%

```{r}
#tc = train_clean
missing_values_tc1 <- map(.x = train_clean, .f = find_na) %>% 
  unlist() %>% 
  data.frame() #Assign dataframe to a new variable called missing values tc1 

missing_values_tc1 <- missing_values_tc1 %>%
  rownames_to_column(var = "Column") %>% 
  rename(Missing_Values = 1) # Rename the unnamed column to "Missing_Values"

missing_values_tc1 %>% 
  filter( . != 0) %>% # Filter to only include rows that are not zero
  summarise(missing_col_cnt  = length(.), # See which columns are missing values
            missing_col_pctg = (length(.)/122) * 100) %>% round(2) # Convert this to a pctg.

missing_values_tc1 %>% 
  filter( . != 0) %>% # Filter to only include rows that are not zero
  mutate(col_pctg_missing_values = ./nrow(train_clean)) %>% # Get this a percentage
  filter(col_pctg_missing_values >=.50) %>% #Filter to only include rows greater than zero
  select(Missing_Values,col_pctg_missing_values) %>% # Select relevant columns
  arrange(desc(col_pctg_missing_values)) # Arrange in descending order


```

The only two variables left that are greater than 50% are OWN_CAR_AGE and EXT_SOURCE_1.

### Own Car Age Missing Values

```{r}

train_clean %>% 
  select(FLAG_OWN_CAR,OWN_CAR_AGE) %>% # Select the following two variables
  filter(FLAG_OWN_CAR == "N") %>% # Only select rows where client marked "N".
  head() # Display the first 6 observations


```

The `OWN_CAR_AGE` variable is closely associated with the `FLAG_OWN_CAR` variable. The `FLAG_OWN_CAR` variable indicates whether someone owns a car or not. This however means that if someone does not own a car they are marked with "N" which should be a zero in the `OWN_CAR_AGE` variable.

This however is not the case. Instead anyone that does not own a car is marked as NA which is inaccurate because they very likely don't own a car. Thus, any NA_Values in the `OWN_CAR_AGE` variable will be imputed as zero.

The above data frame displays this where anybody who marked "no" for owning a car shows an NA value in the `OWN_CAR_AGE` variable. First 6 rows have been showed for demonstration purposes.

```{r}

# Replace missing values in the specific column
train_clean$OWN_CAR_AGE[is.na(train_clean$OWN_CAR_AGE)] <- 0

# Check that the imputation is correct
sum(is.na(train_clean$OWN_CAR_AGE))

```

Any NA Value in the car data set has now been imputed with zero. The code output also reflect that this was successful, since it shows zero which means that there are no missing values in the NA_Column.

### EXT_SOURCE_1 Missing Values

```{r}

# b = bureau
# Define the predictors you want to check for correlation
ext_predictors_to_combine <- c("EXT_SOURCE_1",	
"EXT_SOURCE_2", "EXT_SOURCE_3","TARGET")

# Calculate the correlation matrix (ignoring missing values)
ext_predictors_cor_matrix <- cor(train_set_02[ext_predictors_to_combine], use = "complete.obs")

ext_predictors_cor_matrix

# EXT_Source_1 missing values
round(sum(is.na(train_clean$EXT_SOURCE_1))/nrow(train_clean),2)
# EXT_Source_2 missing values
round(sum(is.na(train_clean$EXT_SOURCE_2))/nrow(train_clean),4)
# EXT_Source_3 missing values
round(sum(is.na(train_clean$EXT_SOURCE_3))/nrow(train_clean),2)

```

It should be noted that `EXT_SOURCE_1` is the variable with 56.38% of its values missing. However, there are 2 other variables that correspond to `EXT_SOURCE_1` which are `EXT_SOURCE_2` and `EXT_SOURCE_3`.

These 3 variables all report the same metric which are the normalized external credit scores.

The above code chunk however shows that these variables are not highly correlated. However with the target variable, all the variables have a somewhat weak negative association.

A possible reason is that the external data sources represent different credit score agencies. The credit agency represented by `EXT_SOURCE_1` may have stricter policies which is why it give less  credit scores to clients.

Consequently, this means that the NA's in `EXT_SOURCE_1` could be that particular credit agency refusing to give credit scores. It would also explain why `EXT_SOURCE_1` is missing 56% while `EXT_SOURCE_2` and `EXT_SOURCE_3` are missing far less values. (`EXT_SOURCE_2` $<$ 1%) and (`EXT_SOURCE_3` $<$ 20%).

Moreover, Home Credit's customers are people who cannot utilize traditional finance routes. This is due to having insufficient financial history, etc. This causes some credit-scoring companies like the company represented in `EXT_SOURCE_1` to not give the client's any credit scores.

Thus, for `EXT_SOURCE_1`, any missing values will be imputed with zero.

```{r}

train_clean$EXT_SOURCE_1[is.na(train_set$EXT_SOURCE_1)] <- 0 #Impute all NA's with zero

sum(is.na(train_clean$EXT_SOURCE_1)) # Check that imputation was sucessful. There should be no missing values

```

#### EXT_SOURCE_2 & 3 Missing Values

Since `EXT_SOURCE_1` has already been addressed, it is logical to also address `EXT_SOURCE_2` and `EXT_SOURCE_3` as well.

`EXT_SOURCE_2` and `3` seem to represent credit card scoring companies who are more lenient with clients. `EXT_SOURCE_2` seems to be very generous since less than 1% of its values are missing.

Thus, the information from EXT_SOURCE_2 and 3 will be used to impute the missing values in both columns.

```{r}

# Create a dataset with only the relevant columns
data_subset <- train_clean[, c("EXT_SOURCE_2", "EXT_SOURCE_3")]

# Use mice to impute missing values
imputed_data <- mice(data_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)

# Get the completed dataset
completed_data <- complete(imputed_data)

# Check that the imputation was sucessful and that there are no missing values
sum(is.na(completed_data$EXT_SOURCE_2))
sum(is.na(completed_data$EXT_SOURCE_3))

# Add these imputed columns to the dataset
train_clean$EXT_SOURCE_2 <- completed_data$EXT_SOURCE_2
train_clean$EXT_SOURCE_3 <- completed_data$EXT_SOURCE_3

# Check that the columns were added successfully and that there are no missing values.
sum(is.na(train_clean$EXT_SOURCE_2))
sum(is.na(train_clean$EXT_SOURCE_3))

```

The MICE function from the MICE (Multivariate Imputation by Chained Equations) library has been implemented to combine information from both datasets and impute the missing values for both columns.

It does this by iteratively modelling the `EXT_SOURCE_2` and `EXT_SOURCE_3` values by using the selected variables as predictors. (It will use `EXT_SOURCE_2` and `EXT_SOURCE_3 as predictors`)

Then it will create a predictive model utilizing these predictors to make predictions about the missing values.

The method utilized is pmm which is Predictive Mean Matching (PMM) which is utilized for continuous data to ensure that imputed values are plausible. (This is done by selecting an actual variable from a similar case which would be EXT_Source_2 and 3 for this scenario.)

## Columns less than 50%

```{r}
#tc = train_clean
missing_values_tc01 <- map(.x = train_clean, .f = find_na) %>% # stores result in a variable
  unlist() %>% 
  data.frame() # Assign all the missing values from train_clean to a dataframe

missing_values_tc01 <- missing_values_tc01 %>%
  rownames_to_column(var = "Column") %>% 
  rename(Missing_Values = 1) # Rename the unnamed column to "Missing_Values"

missing_values_tc01 %>% 
  filter( . != 0) %>% #Filter to include non_zero rows 
  mutate(col_pctg_missing_values = ./nrow(train_clean)) %>% #Calculate percentage which are the NA's for each variable divided by the number of rows in df
  filter(col_pctg_missing_values <.50) %>% #Filter to only include values less than .50
  select(Missing_Values,col_pctg_missing_values) %>% # Select only relevant columns
  arrange(desc(col_pctg_missing_values)) # Arrange these values in descending order

```

The above code displays which columns are missing less than 50% of their values.

The first 3 columns are still missing several values however:

- `FLOORSMAX_MEDI`
- `YEARS_BEGINEXPLUATATION_MEDI`
- `OCCUPATION_TYPE`

`FLOORSMAX_MEDI` and `YEARS_BEGINEXPLUATATION_MEDI` are very similar to other variables that were missing more than 50% of their values. All of these columns describe aspects of a client's house.

### FLOORSMAX_MEDI Missing Values

```{r}

# Get all columns that end with _MEDI
MEDI_COL <- train_clean %>% 
  select(ends_with("_MEDI")) %>%
  colnames()

# Define the predictors to check for correlation
floor_combine <- c("FLOORSMAX_MEDI","FLOORSMIN_MEDI")
overall_combine <- c(floor_combine,MEDI_COL)
# Calculate the correlation matrix (ignoring missing values)
fl_correlation_matrix <- cor(train_clean[overall_combine], use = "complete.obs")

# Extract only the correlations for FLOORSMAX_MEDI
fl_max_correlations <- fl_correlation_matrix["FLOORSMAX_MEDI", ]

# Display the correlations
fl_max_correlations

```

Most of the following variable's correlations with `Floorsmax` are somewhat strongly associated except for the following:

- `NONLIVINGAPARTMENTS_MEDI`
- `YEARS_BEGINEXPLUATATION_MEDI`
- `ENTRANCES_MEDI`

Thus, these variables will be excluded when performing imputation.

#### Impute FLOORSMAX_MEDI missing values

```{r}

# Names to exclude
exclude_names <- c("NONLIVINGAPARTMENTS_MEDI", "YEARS_BEGINEXPLUATATION_MEDI", "ENTRANCES_MEDI")

# Extract names of variables from the correlations, excluding the ones in 'exclude_names'
filtered_names <- setdiff(names(fl_max_correlations), exclude_names)

# Print the filtered names
filtered_names


# Create a dataset with only the relevant columns
fl_data_subset <-train_clean[,filtered_names]

# Use mice to impute missing values
fl_imputed_data <- mice(fl_data_subset, m = 1, method = 'pmm', maxit = 5, seed = 123)

# Get the completed dataset
fl_completed_data <- complete(fl_imputed_data)

# Insert new imputed column back into train dataset.
train_clean$FLOORSMAX_MEDI <- fl_completed_data$FLOORSMAX_MEDI

# Check that imputation was sucessful.
sum(is.na(train_clean$FLOORSMAX_MEDI))

```

The MICE function from the MICE package can be used again to impute the missing values. MICE will iteratively model the FLOORS_MAX_MEDI value by using the selected variables as predictors.

Then it will create a predictive model utilizing these predictors to make predictions about the missing values.

### YEARS_BEGINEXPLUATATION_MEDI

```{r}

# Define the predictors to check for correlation
yr_beg_exp <- c("YEARS_BEGINEXPLUATATION_MEDI")
yr_overall_combine <- c(yr_beg_exp,MEDI_COL)
# Calculate the correlation matrix (ignoring missing values)
yr_correlation_matrix <- cor(train_clean[MEDI_COL], use = "complete.obs")

# Extract only the correlations for FLOORSMAX_MEDI
yr_correlations <- yr_correlation_matrix["YEARS_BEGINEXPLUATATION_MEDI", ]

# Display yr_correlations
yr_correlations

```

The `YEARS_BEGINEXPLUTATION_MEDI` will be checked with the other `MEDI` columns to see if there is any significant correlation. The `MODE` and `AVG` columns have already been dropped. See the "Drop the Non-Median Columns" Header. Moreover, the `MEDI` columns will be the most similar to the `YEARS_BEGINEXPLUTATION_MEDI` column due to the variables measuring the median of different aspects of the house.

The correlation between this `YEARS_BEGINEXPLUATATION_MEDI` and the other variables is fairly weak. Additionally, this variable is missing a lot of information at 48.78%. Thus, this variable may not contain much information. There are additionally several other columns which describe various aspects of a client's house. Due to these reasons, this variable will be dropped from the dataset.

### Drop YEARS_BEGINEXPLUATATION_MEDI

```{r}
#Drop the Year_Beginexpluation variable from the dataframe
train_clean <- train_clean %>% 
  select(-YEARS_BEGINEXPLUATATION_MEDI)

```

`YEARS_BEGINEXPLUATATION_MEDI` has been dropped

### OCCUPATION_TYPE Missing Values

```{r}
# Show the unique values in the occupation type variable
unique(train_clean$OCCUPATION_TYPE)

```

The above code displays all the values for `OCCUPATION_TYPE`. NA however appears if the client's occupation is unknown. This however could be inaccurate because Home Credit serves clients who can't get financing through the regular finance system. Thus, NA could instead represent clients who are currently not employed. Hence, NA will be changed to "unemployed" in the dataset.

### Convert NA to Unemployed in Occupation Type

```{r}
# Convert any NA Values to Unemployed in Occupation Type
train_clean$OCCUPATION_TYPE[is.na(train_set$OCCUPATION_TYPE)] <- "Unemployed"

# Check to see that this was successful.
sum(is.na(train_clean$OCCUPATION_TYPE))

# Convert variable to a factor
train_clean$OCCUPATION_TYPE <- as.factor(train_clean$OCCUPATION_TYPE)

```

The missing values have been successfully imputed with Unemployed and the `OCCUPATION_TYPE` variable has been converted into a factor.

### Remaining Missing Values

```{r}
#tc = train_clean
missing_values_tc02 <- map(.x = train_clean, .f = find_na) %>% # Assign all the reamining missing values to a new df
  unlist() %>% 
  data.frame() #Convert this to a dataframe

missing_values_tc02 <- missing_values_tc02 %>% # Override df with new changes
  rownames_to_column(var = "Column") %>% 
  rename(Missing_Values = 1) # Rename the unnamed column to "Missing_Values"

missing_values_tc03 <- missing_values_tc02 %>% 
  filter( . != 0) %>% #Filter to only include non-zero values 
  mutate(col_pctg_missing_values = ./nrow(train_clean)) %>% # Calculate the pctg of missing values. missing values in columnn divided by number of rows in df
  filter(col_pctg_missing_values <.50) %>% #Filter the values to anything less than .50
  select(Missing_Values,col_pctg_missing_values) %>% # Select relevant columns
  arrange(desc(col_pctg_missing_values)) # Arrange in descending order

missing_values_tc03 # Display the new dataframe
```

All the other columns are missing less than 1% of their values. This is really small which means that imputation with the median will suffice.

```{r}

# Step 1: Extract column names with missing values from missing_values_tc03
columns_to_impute <- missing_values_tc03$Missing_Values

# Step 2: Perform median imputation on these columns in the main dataset
train_clean[columns_to_impute] <- lapply(train_clean[columns_to_impute], function(x) {
  # Replace missing values with the median of each column
  ifelse(is.na(x), median(x, na.rm = TRUE), x)
})

# Step 3: Check if the missing values are imputed
sum(is.na(train_clean[columns_to_impute]))  # This should return 0 if imputation was successful

```

All the remaining columns that had less than 1% of the missing data have been imputed with the median.

## Check Overall Missing Values

```{r}
#tc = train_clean
missing_values_tc03 <- map(.x = train_clean, .f = find_na) %>%
  unlist() %>% 
  data.frame() 

# Display the new dataframe which should have zero missing values
missing_values_tc03


```

The cleaning was successful, there are no missing values in any of the columns now.

# Outliers

```{r}

# Mutate any character variables into factors
train_clean <- train_clean %>% mutate_if(is.character, as.factor)

# Numeric Variables which should be factors stored as a vector
cat_values <- c("TARGET","FLAG_EMP_PHONE","FLAG_WORK_PHONE","FLAG_EMAIL","FLAG_PHONE",'REG_REGION_NOT_WORK_REGION','LIVE_CITY_NOT_WORK_CITY',"REG_CITY_NOT_LIVE_CITY","REG_CITY_NOT_WORK_CITY","REGION_RATING_CLIENT_W_CITY","FLAG_DOCUMENT_3","FLAG_DOCUMENT_6","FLAG_DOCUMENT_8","REGION_RATING_CLIENT")

# Utilize mutate to transform all the columns in the cat_values vector to factors.
train_clean <- train_clean %>% 
  mutate(across(all_of(cat_values), as.factor))

# Display the summary of train_clean
summary(train_clean)

```

Utilize the summary command to see all the variables and see if any potential outliers exist.

Following variables have values that are considered strange. All these columns have values that are excessively large. For instance, `CNT_CHILDREN` has a maximum value of 19 while `OWN_CAR_AGE` has a max value of 91. Each variable and its values will be discussed in its respective section. This is just meant to provide an overview of the variables.

- `CNT_CHILDREN`
- `OWN_CAR_AGE`
- `CNT_FAM_MEMBERS`
- `AMT_INCOME_TOTAL`
- `AMT_CREDIT`
- `AMT_GOODS_PRICE`
- `OBS_60_CNT_SOCIAL_CIRCLE`
- `OBS_30_CNT_SOCIAL_CIRCLE`
- `DEF_60_CNT_SOCIAL_CIRCLE`
- `DEF_30_CNT_SOCIAL_CIRCLE`

**Things to Address:**

- `AMT_REQ_CREDIT_BUREAU_QRT`
- `AMT_REQ_CREDIT_BUREAU_YEAR`
- `AMT_REQ_CREDIT_BUREAU_MON`
- `DAYS_BIRTH`
- `DAYS_REGISTRATION`
- `DAYS_ID_PUBLISH`
- `Gender: XNA`

Before manually removing the rows, the Local Outlier Factor(LOF) algorithm will be utilized from the Library Dbscan. The LOF algorithm measures the local density deviation of a data point relative to its neighbors(cluster). The number of data points are used to determine a cluster or neighborhood. The number of datapoints itself is controlled by the minPts parameter. (If minPts is set to 3, then the 3 nearest neighbors will be utilized to determine if an observation is an outlier. A higher minPt will search more of the dataset while a lower minPt will do a smaller, more localized search.)

A point is considered an outlier if its density(LOF Score) is significantly lower than its neighbors. This is determined by selecting a threshold score. If the LOF Score is a higher than the threshold score, then the observation is marked as an outlier.

The threshold is an arbitrary number determined by us, but there are some guidelines:

LOF Values Around 1:

-   For most data points that are not outliers, the LOF score will be close to 1.
-   LOF scores of 1 indicate that the point is in a region of similar density as its neighbors, so itâ€™s not considered an outlier.

LOF Values $>$ 1:

-   LOF scores greater than 1 indicate potential outliers. The larger the score, the more likely the point is an outlier.
-   Common thresholds for identifying outliers are LOF scores between 1.5 to 2 and higher.

Typical Thresholds:

-   Threshold between 1.5 and 2: This range often works well for flagging moderate outliers.

This also means that a lower threshold will result in the LOF algorithm classifying more observations as outliers. (Observation's LOF Score must be greater than the outlier.) This also means that a higher threshold will result in the LOF algorithm classifying less observations as outliers.

## LOF Alogrithm

```{r}

threshold <- 1.5 #Determined by us

numeric_columns <- sapply(train_clean, is.numeric) # Get all numeric columns
train_clean_numeric <- train_clean[, numeric_columns] # Use Nuemric Columns to subset train_clean and push changes to new variable.


lof_scores <- lof(train_clean_numeric, minPts = 6) # Set up LOF Algorithm
outliers <- train_clean[lof_scores > threshold, ] # If the lof_score is greater than the threshold score, then the row is considered an outlier.

train_clean_filtered <- train_clean[lof_scores <= threshold, ] # Filter train_clean filtered to only include non_outliers. If a row has an lof score that is greater than the threshold, it's considered an outlier. However, this means that if a row has an LOF score that is less than the threshold, it's considered an observation. 

# Get the row indices of the filtered dataset
filtered_indices <- rownames(train_clean_filtered)

# Use these indices to filter the original dataset
train_clean <- train_clean[filtered_indices, ]

# Calculate how many rows have been taken away from the train_clean dataset. 
row_reduction <- ((307511 - nrow(train_clean))/307511) * 100

# Round this output to 2
round(row_reduction,2)

# Check the output for train_clean again via summary()
summary(train_clean)

```

I was relatively conservative with how many rows were eliminated from this dataset since I picked a moderate threshold of 1.5 for determining outliers. The amount of neighbors (minPts) used to determine each algorithm was also relatively small at 6. This consequently resulted in only 2.6% of the rows being eliminated. (The dataset has gone from 307,511 rows to 299,311 rows.)

Several of the extreme values are still present which will require manual cleaning such as `OWN_CAR_AGE` which shows 91 years.

## CNT_CHILDREN

`CNT_CHILDREN` has a max value of 19. This is a very unrealistic value. Thus, it'll be filtered.

```{r}

train_clean <- train_clean %>% #Overwrite dataset with the changes
  filter(CNT_CHILDREN <= 7) #7 is a fairly large number that takes into account how many children a family could have without being too unrealistic.

summary(train_clean$CNT_CHILDREN) # Call summary to check the rest of hte values and the cnt_children as well.

```

This has removed some rows. The train_clean data set has gone from 299,331 rows to 299,317 rows.

## OWN_CAR_AGE

The oldest car in the dataset is 91 years old which seems very implausible. Thus, this value will be filtered out based on the average age of a car which is 12.43 yrs per this article:

https://www.caranddriver.com/news/a60882953/average-age-us-cars-trucks-suvs-rises/

```{r}

train_clean <- train_clean %>% # OVerride df with new changes
  filter(OWN_CAR_AGE < 12.43) # Filter to only include cars with an average age less than 12.43 years.

```

The number of rows has gone from 299,317 rows to 262,483 rows.

## CNT_FAM_MEMBERS

Some people in this dataset have been recorded to have 20 family members. This however seems very unlikely, since the median is 2. This also indicates that most clients may have only put their immediate family such as their spouse and family. Thus, the max value will be reduced to 5 to indicate for any extra immediate family members like extra children.

```{r}

train_clean <- train_clean %>% # OVerride df with new changes
  filter(CNT_FAM_MEMBERS < 5) # Filter to only inclue families with less than 5 people 

```

The number of rows has decreased from 262,483 to 259,293 rows.

## AMT_INCOME_TOTAL

117,000,000 was listed as the maximum income for someone in this dataset. This value however seems very unlikely in the context of this case. Home Credit is lending to people who typically can't afford conventional financing. A client with an income of 117,000,000 would probably not need to utilize Home Credit's services. It additionally is very far away from the 3rd quartile which is only 202,500 dollars. Thus, to address this issue, a value relatively close to the 3rd quartile will be selected to filter the data.

```{r}

train_clean <- train_clean %>% # Override df with new changes
  filter(AMT_INCOME_TOTAL < 300000) # Filter the income to be less than 300k 

```

The number of rows has reduced from 259,293 rows to 239,985 rows.

```{r}



# Count how many values in 'AMT_CREDIT' are greater than or equal to 1,000,000
sum(train_clean$AMT_CREDIT >= 1000000)

# Count how many values in 'AMT_CREDIT' are less than 1,000,000
sum(train_clean$AMT_CREDIT < 1000000)

# Count how many values in 'AMT_CREDIT' are greater than or equal to 1,000,000
sum(train_clean$AMT_GOODS_PRICE >= 1000000)

# Count how many values in 'AMT_CREDIT' are less than 1,000,000
sum(train_clean$AMT_GOODS_PRICE < 1000000)


```

Upon further inspection, it has been decided to not remove `AMT_CREDIT` and `AMT_GOODS_PRICE`. This is because `AMT_CREDIT` has values that are greater than 1,000,000 dollars which comprise 14% of the dataset. If these rows are removed, then too much data will be lost. Additionally, the high volume of rows suggests that these are legitimate observations. Moreover, `AMT_GOODS_PRICE` is closely associated with `AMT_CREDIT`, which means that a closer inspection of these two variables will be required.

## AMT_CREDIT and AMT_GOODS_PRICE

The maximum value for both of these variable is 4050000 which indicates that some people may have gotten a loan for 400,000 dollars (AMT_CREDIT) and then purchased something for 405,000 dollars. This relationship however seems to have been altered by removing some of the rows from AMT_INCOME_TOTAL.

```{r}

summary(train_clean$AMT_CREDIT) # Get a summary of just AMT_Credit
summary(train_clean$AMT_GOODS_PRICE) # Get a summary of just AMT_Goods_Price

credit_col <- c("AMT_CREDIT", "AMT_GOODS_PRICE") # Combine these two columns into a vector

# Calculate the correlation matrix (ignoring missing values)
correlation_matrix_credit <- cor(train_clean[credit_col], use = "complete.obs")

# Display correlation matrix
correlation_matrix_credit


```

The maximum value for `AMT_CREDIT` is now 3,860,019 while the goods price is 3,555,000. These values are still relatively high, which may require removal. However too many rows should not be removed. Hence, a more generous threshold will be utilized.

Additionally both of these columns are highly correlated with each other per the correlation matrix. This means that these values are highly associated and removing values from one column will affect the other column. Hence, due to this reason, `AMT_GOODS_PRICE` will also not have any values removed.

## OBS_60_CNT_SOCIAL_CIRCLE, DEF_60_CNT_SOCIAL_CIRCLE, OBS_30_CNT_SOCIAL_CIRCLE, DEF_30_CNT_SOCIAL_CIRCLE

These variables shows how many people in the Client's social circle had late payments or defaulted. The maximum value for `OBS_60_CNT_SOCIAL_CIRCLE` is 344.000 which indicates that one client had 344 people in their social circle who made late payments.

```{r}

train_clean %>% 
  filter(OBS_60_CNT_SOCIAL_CIRCLE == 344) # Filter to only include the maximum values from OBS_60


```

Upon further inspection, it appears that the maximum values for the following variables are also associated with this client:

- `DEF_60_CNT_SOCIAL_CIRCLE` (24)
- `DEF_30_CNT_SOCIAL_CIRCLE` (34)
- `OBS_30_CNT_SOCIAL_CIRCLE` (348)
- `OBS_60_CNT_SOCIAL_CIRCLE` (344)

These values overall suggest that all of the extreme values are associated with only one client. This means that this client is a very extreme observation. Interestingly, the client was able to pay back their loan. However, regardless of their default status, this client is still an extreme observation that only appears once in the dataset. Thus, this row will be removed which will also remove the extreme values for the other columns as well:

- `OBS_30_CNT_SOCIAL_CIRCLE`
- `DEF_30_CNT_SOCIAL_CIRCLE`
- `DEF_60_CNT_SOCIAL_CIRCLE`

```{r}

train_clean <- train_clean %>% 
  filter(OBS_60_CNT_SOCIAL_CIRCLE != 344) # Filter to include all rows that do not equal 344



```

This has removed a row from the train_clean dataset. The overall number of rows is 239,984.

### Check OBS_60_CNT_SOCIAL_CIRCLE, DEF_60_CNT_SOCIAL_CIRCLE, OBS_30_CNT_SOCIAL_CIRCLE, DEF_30_CNT_SOCIAL_CIRCLE

```{r}

# Get summaries for just the selected variables by utilizing dollar sign notation.
summary(train_clean$OBS_30_CNT_SOCIAL_CIRCLE)
summary(train_clean$DEF_60_CNT_SOCIAL_CIRCLE)
summary(train_clean$DEF_30_CNT_SOCIAL_CIRCLE)
summary(train_clean$OBS_60_CNT_SOCIAL_CIRCLE)

train_clean %>% 
  filter(OBS_60_CNT_SOCIAL_CIRCLE == 47,
         OBS_30_CNT_SOCIAL_CIRCLE == 47) # Filter to only include 47 in both OBS_60 and OBS_30

```

There are still some extreme values in the `OBS_30_CNT_SOCIAL_CIRCLE` and the `OBS_60_CNT_SOCIAL_CIRCLE` variables. This indicates that a stricter threshold is required for removing the remaining outlier values.

```{r}
train_clean %>%
  filter(OBS_30_CNT_SOCIAL_CIRCLE <= 10,
         OBS_60_CNT_SOCIAL_CIRCLE <= 10) # Filter to only include 10 in both OBS_30_CNT and OBS_60_CNT

train_clean <- train_clean %>% # Same Code but the changes are now being officially piped to the train_clean df
  filter(OBS_30_CNT_SOCIAL_CIRCLE <= 10,
         OBS_60_CNT_SOCIAL_CIRCLE <= 10)

# Utilize the summary command to check the distributions for the OBS_30 and OBS_60 Variables.
summary(train_clean$OBS_30_CNT_SOCIAL_CIRCLE)
summary(train_clean$OBS_60_CNT_SOCIAL_CIRCLE) 

summary(train_clean$DEF_60_CNT_SOCIAL_CIRCLE)
summary(train_clean$DEF_30_CNT_SOCIAL_CIRCLE)

```

This distributions for Client's Social Circle Variable look more reasonable. It also appears that the same extreme observation occurred in `OBS_30` and `OBS_60`. 207 rows were removed due to this procedure which is less than 1% of the train_clean dataset. (The train_clean dataset had 239,983 rows prior to removal and now has 237,776 rows.)

## Check Summary Values Again

```{r}
# Check all of the variables
summary(train_clean)

```

The cleaning has been successful, there are no observable extreme values. Additionally, other variables that may have had some extreme values have also been indirectly addressed such as:

- `AMT_REQ_CREDIT_BUREAU_QRT`, old: 261, new: 19

Although this value is still high, it's much closer to the maximum of the other variables like `AMT_REQ_CREDIT_BUREAU_YEAR` (25) and `AMT_REQ_CREDIT_BUREAU_WEEK` (24). Thus, further removal of outliers does not seem required for the `AMT_CREDIT_BUREAU_QRT` column.

It should be noted that while `BUREAU_QRT` is close to the max of the other BUREAU Columns, the values are still high when compared to the median, 3rd quarter, etc. For instance, the median for `AMT_REQ_CREDIT_BUREAU_YEAR` is 3 inquires while the max is 25 inquires. `AMT_REQ_CREDIT_BUREAU_MON` also displays a similar trend with the max being 24 inquiries while the median is 1 inquiry.

These are however plausible values which could reflect Home Credit's concern about particular clients. In other words those clients may have had concerning characteristics that warranted more inquires then usual. The exception however would the be the old `AMT_REQ_CREDIT_BUREAU_QRT` which was 261 inquiries. This seems like an extreme value which warranted removal.

# Other Variable Issues

The following 4 variables have the distribution of their variables reversed:

-   DAYS_BIRTH
-   DAYS_REGISTRATION
-   DAYS_ID_PUBLISH
-   DAYS_LAST_PHONE_CHANGE

For instance, the below code for DAYS_BIRTH displays this issue.

```{r}
# Show the summary of Days_Birth.
summary(train_clean$DAYS_BIRTH)

round(25201/365,2) # Calculate the age of the oldest client in this dataframe. 
round(7673/265,2)  # Calculate the age of the youngest client in this dataframe. 
```

Days birth measures how many days it has been since the client was born. This is done by subtracting the birth date measured as 0 from the current client's age in day. So if a client is 69.04 years, their birthdate is calculated as follows:

0-25201 = -2501 days or -69.04 years. This however is an issue if the client is 69 years old, because the summary command has marked the client as the minimum value. The 69 year old client however should be the maximum while the -2501 client who is 29.85 years should be marked as the youngest.

Thus, to fix this issue, the variable distribution will be reordered.

```{r}

# Convert DAYS_BIRTH to positive values with absolute value (to represent age in days)
train_clean$DAYS_BIRTH <- abs(train_clean$DAYS_BIRTH)
summary(train_clean$DAYS_BIRTH) # Show summary of Days_Birth.

```

The other variables have the same issue:

-   `DAYS_REGISTRATION`
-   `DAYS_ID_PUBLISH`
-   `DAYS_LAST_PHONE_CHANGE`

They all represent how many days it's has been since the client did something prior to the application.

## DAYS_REGISTRATION

For instance, `DAYS_REGISTRATION` measures how many days prior to the application did client change his registration. However the datset has measured 0 days as the max which is incorrect. 24,672 days should be the max which means that the client changed his registration 24,672 days before the application. 0 days would likewise mean that the client did not change his registration prior to the application.

```{r}
# Show summary of just Days_registration
summary(train_clean$DAYS_REGISTRATION)
train_clean$DAYS_REGISTRATION <- abs(train_clean$DAYS_REGISTRATION) # Convert values to positive distribution with absolute values
summary(train_clean$DAYS_REGISTRATION) # Show updated summary of just Days_Registration
```

Days_Registration has been successfully reordered with 24672 days reflecting the maximum number of days a client changed their registration prior to their loan application.

## DAYS_ID_PUBLISH

`DAYS_ID_PUBLISH` is how many days is how many days before the application did the client change the identity document that he used to apply for the loan. The issue however is that maximum number of days before a client changed their document is 0.

This is incorrect because 0 should be the minimum which means that the minimum number of days before a client changed their document is 0. If minimum is zero though, this means that the client did not change their document at all.

```{r}

summary(train_clean$DAYS_ID_PUBLISH) # Show summary of just DAYS_ID_PUBLISH
train_clean$DAYS_ID_PUBLISH <- abs(train_clean$DAYS_ID_PUBLISH) # Convert values to reverse distribution with absolute values
summary(train_clean$DAYS_ID_PUBLISH) #Show updated summary of just DAYS_ID_PUBLISH

```

Days_ID_Publish has been successfully reordered with 7197 days reflecting the maximum number of days a client changed the ID Document that they used to apply for the loan.

## DAYS_LAST_PHONE_CHANGE

`DAYS_LAST_PHONE_CHANGE` represents how many days before the application did the client change their phone. The problem is that smallest amount of days and the largest amount of days are reversed. For instance, the min is 4,292 which means that the client changed their phone 4,292 days before the application. This however should be the max which would be the maximum number of days before an application. Thus, the column's values need to be reordered.

```{r}

summary(train_clean$DAYS_LAST_PHONE_CHANGE) # Show summary of just DAYS_LAST_PHONE_CHANGE
train_clean$DAYS_LAST_PHONE_CHANGE <- abs(train_clean$DAYS_LAST_PHONE_CHANGE) #Convert values to reverse distribution with absolute values 
summary(train_clean$DAYS_LAST_PHONE_CHANGE) #Show updated summary of just DAYS_LAST_PHONE_CHANGE

```

The columns values have been successfully reordered with the maximum number of days being 4,292 days and the minimum number of days being 0 days. (This means if someone changed their phone 4,292 days before the application, it will be recorded as the maximum.)

## Check Overall Values Again

```{r}

summary(train_clean) # Check all the overall values again to ensure that the observations look reasonable.

```

The overall values look good, however there are 2 columns that have the same issue:

-   `ORGANIZATION_TYPE`
-   `CODE_GENDER`

Both of these values have a category called XNA. "XNA" however means different things in the context of both variables. Since these columns had no missing values (NA), R did not classify these columns as containing missing values. Hence, this must also be fixed.

### ORGANIZATION_TYPE

`ORGANIZATION_TYPE` captures which industry the client works in. There is a column called "XNA" however. XNA could represent two things:

-   The client genuinely did not provide the industry that they worked in. Thus, XNA represents the missing industry.

-   XNA still represents the missing value but the missing value means something different. There are 58 unique levels in the `ORGANIZATION_TYPE` variable, including an "other" category. This means that even if the client's industry was not listed, they could have selected "other". Additionally, there is no "unemployed" category. XNA also represents 20% of the missing values in the `ORGANIZATION_TYPE` which does not seem Missing Completely at Random. Thus, XNA will be converted to "unemployment".

```{r}

unique(train_clean$ORGANIZATION_TYPE) # See the unique values in Organization_Type
round(sum(train_clean$ORGANIZATION_TYPE == 'XNA')/nrow(train_clean),2) * 100 # Calculate what percentage of the columns is XNA

# Replace the XNA Column with Unemployed
levels(train_clean$ORGANIZATION_TYPE)[levels(train_clean$ORGANIZATION_TYPE) == "XNA"] <- "Unemployed"

# Check that this worked sucessfully
levels(train_clean$ORGANIZATION_TYPE)


```

The conversion was successful and XNA has now been changed to Unemployed which can be seen from the new levels for the `ORGANIZATION_TYPE` variable.

### CODE GENDER

The `Gender` Variable has a valued called XNA. This could represent 2 things:

-   XNA are clients who did not provide their gender to Home Credit or Home Credit was unable to obtain their gender through other means. In other words, these are genuinely missing values.

-   The client may not be a "female" or "male". They may identify as some other gender. Thus, they decided to put anything for gender since "other" may not have been an option.

```{r}

unique(train_clean$CODE_GENDER) # See the unique values in CODE_GENDER
round(sum(train_clean$CODE_GENDER == 'XNA')/nrow(train_clean) * 100,4) # Calculate what percentage of the columns is XNA

# Show the actual count of missing values
sum(train_clean$CODE_GENDER == 'XNA')

```

Upon closer inspection, XNA represents less than 1% of the values in the Gender_Count Column. (There are only 3 XNA values overall). Thus for simplicity, this column can be imputed with the mode. The XNA values could be converted to "other" but they comprise a small portion of the column. This means that their predictive power will be limited in comparison to the "Male" and "Female" genders.

```{r}

gender_var <- find_mode(as.character(train_clean$CODE_GENDER)) # Use mode function to calculate which gender should be the mode and assign to variable

levels(train_clean$CODE_GENDER)[levels(train_clean$CODE_GENDER) == "XNA"] <- gender_var # Impute the missing values with the mode

levels(train_clean$CODE_GENDER) # Check that the conversion worked with levels command

```

The conversion was successful, gender only has 2 values, Male and Female.

# Visualizations

Visualizations have been created to help answer the exploratory quesetions. The visualizations will also help wtih enabling exploration of interesting variables.

## Bar Plots

### Distribution of Default Status by Contract Type

Is there a relationship between a client's contract type and default status? Perhaps certain Contract Types make it harder to pay the loan back.

```{r}

# Count Visualization
ggplot(data = train_clean, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) + 
  geom_bar(position = 'dodge') + # plot type
  labs(title = "Distribution of Default Status by Contract Type", 
       x = "Contract Type", # x-axis
       fill = "Default Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Specify the theme
  coord_flip() # Flip the graph

# Percentage Visualization
ggplot(data = train_clean, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) + #Plot Name_Contract_Type
  geom_bar(position = 'fill') + #bars filled in with percentages
  labs(title = "Distribution of Default Status by Contract Type", #custom plot title
       x = "Contract Type", # custom x-axis title
       fill = "Default Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),  # Assign custom colors
                    labels = c("No Default", "Defaulted")) +        # Custom labels for the legend  
  theme_minimal() + # Specify the theme
  coord_flip() + # Flip the graph
  scale_y_continuous(labels = scales::percent_format()) # Convert counts to pctg.

```

The above plot shows the distribution of Default Status by Contract Type. The first plot shows the count of default status for both revolving and cash loans.

The second plot shows the percentage of default status for each of the two groups which shows a few things. This means that out of all the Revolving Loans issued, the percentage of customers who defaulted is approximately 5% while the customers who did not default is approximately 95%. Likewise out of all the the Cash Loans issued, the percentage of customers who defaulted is approximately 11%, while the customers who did not default is approximately 89%. This means that that overall percentage of default is lower for the revolving loans (5%) when compared to the cash loans (11%).

### Median Annuity Amount vs Default Status

Is there a relationship between Median Annuity Amount and Default Status? It could be the case that a lower Median Annuity Amount is easier to pay off. Thus, lower Median Annuity Amount may be lower for those who did not default on their loans.

```{r}

train_clean %>%
  group_by(TARGET) %>% #Group the variables by target
  summarise(median_amt_annuity = median(AMT_ANNUITY)) %>% #Calculate median amt annuity for each level of target
  ggplot(mapping = aes(x = TARGET, y = median_amt_annuity, fill = TARGET)) + 
  geom_col(position = 'dodge') + # Specify plot type
  labs(title = "Median Annuity Amount vs Default Status", # specify chart title
       x = "Default Status", # x-axis title
       y = "Median Annuity Amount", # y-axis title
       fill = 'Default Status') +
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +
  theme_minimal() + # Specify plot theme
  theme(axis.title.y = element_text(margin = margin(t = 50))) + # Adjust top margin of y-axis label
  coord_flip() # Flip graph 


```

There does not appear to be a strong relationship between Annuity Amount and Default Status. The default and no default bars are very close to each other. The Median Annuity Amount for Defaulted is slightly larger than No Default but not by much.

### Median Credit Amount vs Default Status

Is there a relationship between a client's Median Credit Amount and Default Status? It could be possible that clients with higher median credit have lower default rates. This is compared to clients who have lower Median Credit.

```{r}

train_clean %>%
  group_by(TARGET) %>% #Group the variables by target
  summarise(median_credit_amt = median(AMT_CREDIT)) %>% #Calculate median amt annuity for each level of target
  ggplot(mapping = aes(x = TARGET, y = median_credit_amt, fill = TARGET)) + 
  geom_col(position = 'dodge') + #Specify plot type
  labs(title = "Median Credit Amount vs Default Status", # specify title
       x = "Default Status", # x-axis title
       y = "Median Credit Amount", # y-axis title
       fill = 'Default Status') +
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +
  theme_minimal() + # Specify plot theme
  theme(axis.title.y = element_text(margin = margin(t = 50))) + # Adjust top margin of y-axis label
  coord_flip() # Flip graph

```

The relationship between Credit Amount and Default Status is also very similar. Interestingly, the Medan Credit Amount for "No Default" is slightly higher than the Credit Amount for "Defaulted".

### Median Amount Goods Price vs Default Status

Is there a relationship between a client's median amount goods price and default status? It could be that a lower median amount goods price results in less default since the client would need a smaller loan to cover the price of the good.

```{r}

train_clean %>%
  group_by(TARGET) %>% #Group the variables by target
  summarise(median_amt_goods_price = median(AMT_GOODS_PRICE)) %>% #Calculate median amt goods price for each level of target
  ggplot(mapping = aes(x = TARGET, y = median_amt_goods_price, fill = TARGET)) + 
  geom_col(position = 'dodge') + # Specify plot type
  labs(title = "Median Amounts Good Price vs Default Status", # specify title
       x = "Default Status", # x-axis title
       y = "Median Amount Goods Price", # y-axis title
       fill = 'Default Status') +
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +
  theme_minimal() + # Specify plot theme
  theme(axis.title.y = element_text(margin = margin(t = 50))) +  # Adjust top margin of y-axis label
  coord_flip() # Flip graph


```

The Median Amount Goods Price is the same for both "No Default" and "Defaulted". This indicates that there is no relationship between Median Amounts Good Price and Default Status.

### Median Total Income vs Default Status

Is there a relationship between a client's median income for default vs no default? It could be that a higher Median Income results in a lower default rate.

```{r}

train_clean %>%
  group_by(TARGET) %>% #Group the variables by target
  summarise(median_income = median(AMT_INCOME_TOTAL)) %>% # Calculate median amt goods price for each level of target
  ggplot(mapping = aes(x = TARGET, y = median_income, fill = TARGET)) + 
  geom_col(position = 'dodge') + # Specify plot type
  labs(title = "Median Total Income Amount vs Default Status",
       x = "Default Status", # x-axis title
       y = "Median Income", # y-axis title
       fill = 'Default Status') +
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +
  theme_minimal() + # Specify plot theme
  theme(axis.title.y = element_text(margin = margin(t = 50))) + # Adjust top margin of y-axis label
  coord_flip() # Flip graph


```

The Median Total Income is the same for both "No Default" and "Defaulted". This indicates that there may not be a relationship between Median Total Income and Default Status.

### Occupation vs Default Status

What is the relationship between a client's occupation and default status? Certain Higher Paying Occupations like IT for instance might have lower rates of default compared to other groups like Watiers/barmen staff for instance. Perhaps, occupations that traditionally pay higher like IT Staff might have lower rates of default.

```{r}

# Count Plot
ggplot(data = train_clean, aes(x = OCCUPATION_TYPE, fill = as.factor(TARGET))) + #plot occupation_type
  geom_bar(position = 'dodge') + # position bars side by side
  labs(title = "Distribution of Default Status by Occupation", # custom plot title
       x = "Occupation Type", # custom x-axis
       fill = "Repayment Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Specify plot theme
  coord_flip() # Flip graph

# Pctg plot
# First, calculate the percentage of defaults by occupation
default_percent <- train_clean %>% #Assign the train_clean dataset to default_percent
  group_by(OCCUPATION_TYPE) %>% #Group the target by occupation type
  summarize(default_rate = mean(as.numeric(TARGET))) # Calculate the mean of the target

# Reorder the occupation based on default rate
train_clean_01 <- train_clean %>% # Assign changes to another new dataset
  mutate(OCCUPATION_TYPE = factor(OCCUPATION_TYPE, 
                                  levels = default_percent$OCCUPATION_TYPE[order(default_percent$default_rate)])) # Add a new level to NAME_EDUCATION_TYPE from teh defaul_percent df

# Now plot with the reordered occupation types
ggplot(data = train_clean_01, aes(x = OCCUPATION_TYPE, fill = as.factor(TARGET))) + 
  geom_bar(position = 'fill') + #bars filled in with percentages
  labs(title = "Distribution of Default Status by Occupation", 
       x = "Occupation Type",  #custom x-axis title
       fill = "Default Status", #change the legend title here
       y = "Percentage") +  # #custom y-axis title
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"), # Custom colors 
                    labels = c("No Default", "Defaulted")) +    # Custom Labels   
  scale_y_continuous(labels = function(x) paste0(x * 100, "%")) + 
  theme_minimal() + # Specify plot theme
  coord_flip() # Flip graph



```

The plots show the relationship between default status and occupation by count and percentage. There are however some slight differences in the graphs due the scaling of the count and percentage axes respectively. For instance, IT and HR Staff show no default on the count plot. They however do have default rates which are reflected on the percentage plot. The reason for this is that the count is much higher for certain groups like Unemployed and ggplot has to scale the count axis to account for the large count size. This unfortunately, however means that smaller groups like IT Staff, etc. have their default count omitted from the graph if it's a small number compared to the rest of the occupation types.

Additionally, in terms of the percentage visualization, Low-skill Laborers have the highest rate of default when compared to their overall group. Low-skill Laborers have a default rate of approximately 20% compared to a no default rate of 80% for the entire group. Afterwards this next group of occupations has a similar level of default:

-   Waiters/barmen staff
-   Drivers
-   Laborers
-   Security Staff
-   Cooking Staff
-   Sales Staff
-   Cleaning Staff

The rate of default for these groups when compared to their overall group is approximately 15% which is less then the Low-Skill Laborers. Afterwards, Reality Agents and Secretaries have the same rate of default which is approximately 12%.

Finally, the rest of the occupations have a default rate that approximately hovers around 5%.

It seems that there is a trend in the default where blue-collar jobs or work that does not necessarily require a degree has a higher rate of default in their overall group when compared to the white-collar jobs.

### Education vs Default Status

What is the relationship between a client's education and default status? Higher Education Status could lead to clients making more income and being able to better pay of loans. Lower Education Status in contrast could result in less income which means that a client may have difficulty paying their loan.

```{r}

# Count Plot
ggplot(data = train_clean, aes(x = NAME_EDUCATION_TYPE, fill = as.factor(TARGET))) + 
  geom_bar(position = 'dodge') + # put the bars sided by side with dodge
  labs(title = "Distribution of Default Status by Education", #custom plot title
       x = "Education", #custom x-axis title
       fill = "Repayment Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Specify plot theme
  coord_flip() # Flip graph


# Percentage Plot
# First, calculate the percentage of defaults by NAME_EDUCATION_TYPE
default_percent <- train_clean %>% # Create a new df to store these results
  group_by(NAME_EDUCATION_TYPE) %>% # Sort the df by NAME_EDUCATION_TYPE
  summarize(default_rate = mean(as.numeric(TARGET))) # convert target to numeric and calculate proportion of default and no default

# Reorder the NAME_EDUCATION_TYPE based on default rate
train_clean_01 <- train_clean %>% # Assign changes to another new dataset
  mutate(NAME_EDUCATION_TYPE = factor(NAME_EDUCATION_TYPE, 
                                  levels = default_percent$NAME_EDUCATION_TYPE[order(default_percent$default_rate)]))# Add a new level to NAME_EDUCATION_TYPE from teh defaul_percent df

# Now plot with the reordered NAME_EDUCATION_TYPE types
ggplot(data = train_clean_01, aes(x = NAME_EDUCATION_TYPE, fill = as.factor(TARGET))) + 
  geom_bar(position = 'fill') + #bars filled in with percentages
  labs(title = "Distribution of Default Status by Education", #custom title
       x = "Education", #custom x-axis title
       fill = "Default Status", #Change the legend title
       y = "Percentage") +  #custom y-axis title
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),  # Custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels
  scale_y_continuous(labels = function(x) paste0(x * 100, "%")) + # Convert cnt to pctg. 
  theme_minimal() +  # Specify plot theme
  coord_flip() # Flip graph
  
```

The first visualization shows the count of default status by education attainment. The second visualization shows the percentage of default status by education attainment. Based on the second plot, there is a decrease in the default rate as education level increases with Higher Education having the lowest default rate.

It should be noted that everyone who has an academic degree was able to pay off their loan. The issue however is that academic degree has relatively few observations when compared to the other educational groups. This can be seen from the first visualization where academic degree does not have as many observations as Secondary/secondary special for instance. Consequently, in order to adequately scale the higher counts for the rest of the groups, Academic Degree been excluded.

This also means that people who have academic degrees may not necessarily get loans from Home Credit very frequently since they probably can get loans from more conventional banks. (A person with an academic degree will likely make more money which means that they can get a loan from a conventional bank. Consequently, this means that the number of people who would get a loan from Home Credit is low which is also why their count is relatively low compared to the other educational groups.)

Nonetheless, academic degree aside, it seems that the groups with higher educational attainment have a a slightly default rate compared to lower educational attainment.

### Family Status vs Default Status

The Family Status could have an impact on a client's ability to pay back loans. For instance, a married couple will generally have more income then an unmarried client. Thus, a married couple may be able to better pay of their loan and avoid default compared to an unmarried client.

```{r}

# COunt Plot
ggplot(data = train_clean, aes(x = NAME_FAMILY_STATUS, fill = as.factor(TARGET))) + # Plot Family_Status
  geom_bar(position = 'dodge') + # bars side by side
  labs(title = "Distribution of Default Status by Family Status", # Custom title
       x = "Family Status", # custom x-axis title
       fill = "Default Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Assign a theme 
  coord_flip() # Flilp the graph


# First, calculate the percentage of defaults by NAME_FAMILY_STATUS
default_percent <- train_clean %>% # Create a new df to store these results
  group_by(NAME_FAMILY_STATUS) %>% # Sort the df by NAME_FAMILY_STATUS
  summarize(default_rate = mean(as.numeric(TARGET))) # convert target to numeric and calculate proportion of default and no default

# Reorder the occupation based on default rate
train_clean_01 <- train_clean %>% # Assign changes to another new dataset
  mutate(NAME_FAMILY_STATUS = factor(NAME_FAMILY_STATUS, 
                                  levels =  default_percent$NAME_FAMILY_STATUS[order(default_percent$default_rate)])) # Add a new level to NAME_FAMILY_STATUS

# Now plot with the reordered occupation types
ggplot(data = train_clean_01, aes(x = NAME_FAMILY_STATUS, fill = as.factor(TARGET))) + #Plot with new dataframe
  geom_bar(position = 'fill') + #bars filled in with percentages
  labs(title = "Distribution of Default Status by Family Status", # custom title
       x = "Family Status", #custom x-axis title
       fill = "Default Status", # Change the legend title here
       y = "Percentage") +  #Custome y-axis title
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),  # Assign custom colors
                    labels = c("No Default", "Defaulted")) +       # Custom labels for the legend   
  scale_y_continuous(labels = function(x) paste0(x * 100, "%")) + # Convert counts to pctg.
  theme_minimal() + #Assign a theme 
  coord_flip() # Flipl the graph


```

The first bar plot shows the relationship between Family Status and Default Status in counts while the second bar plot shows the same relationship in percentages. It appears that the Civil Marriage and Single/not married groups have the highest rate of default compared to the other Family Status groups. (Rate of default for both groups is roughly 10%) Afterwards, Separated and Married have the same rate of default at 9% which is slightly lower. Finally, Widow is slightly lower at 9%.

The Unknown group however has no defaults with every observation successfully paying their loan off. It should be noted that the "Unknown" group has relatively few observations when compared to the other Family Status groups.

This can be seen from the first visualization where the "Unknown" group does not have as many observations as the "Married" group for instance. Consequently in order to adequately scale the much higher counts for the rest of the groups, ggplot has excluded "Unknown" from the first plot.

Finally, it seems like family status may not have an impact on the default rate. For instance, Separated and Married are opposite family classes yet they have the same default rate. Likewise Single/not married and Civil Marriage which could be perceived as opposites also have the same default rate.

### Income Type vs Default Status

What is the relationship between a client's income type and default status? It could be that clients with high paying jobs default less when compared to clients with low paying jobs.

```{r}

# Count Plot
ggplot(data = train_clean, aes(x = NAME_INCOME_TYPE, fill = as.factor(TARGET))) + 
  geom_bar(position = 'dodge') + # side by side bars
  labs(title = "Distribution of Default Status by Income Type", # custom title
       x = "Income Type", #custom x-axis
       fill = "Default Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Assign a theme
  coord_flip() # Flip the graph

# First, calculate the percentage of defaults by NAME_INCOME_TYPE
default_percent <- train_clean %>% # Create a new df to store these results
  group_by(NAME_INCOME_TYPE) %>% # Sort the df by NAME_INCOME_TYPE
  summarize(default_rate = mean(as.numeric(TARGET))) # Calculate default_rate for NAME_INCOME_TYPE

# Reorder the occupation based on default rate
train_clean_01 <- train_clean %>% # Create another dataframe called train_clean_01
  mutate(NAME_INCOME_TYPE = factor(NAME_INCOME_TYPE, #Convert NAME_INCOME_TYPE to a factor
                                  levels =  default_percent$NAME_INCOME_TYPE[order(default_percent$default_rate)])) #Add the default_pct as a new level

# Percentage Plot                                    
# Now plot with the reordered occupation types
ggplot(data = train_clean_01, aes(x = NAME_INCOME_TYPE, fill = as.factor(TARGET))) + # Plot percentage with new visualization
  geom_bar(position = 'fill') + #Fill in bar graph w/ pctg.
  labs(title = "Distribution of Default Status by Income Type", # custom title
       x = "Income Type", #custom x-axis title
       fill = "Default Status", #change the legend title here
       y = "Percentage") +  #custom y-axis title
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),  #Assign custom colors
                    labels = c("No Default", "Defaulted")) +      # Custom labels for the legend    
  scale_y_continuous(labels = function(x) paste0(x * 100, "%")) + # convert cnt to pctg.
  theme_minimal() + # Assign a theme 
  coord_flip() # Flip the graph

```

The first plot shows how the default status varies by "income type" in counts while the second plot shows how default status the same relationship as percentages. Additionally, the second bar graph shows that Maternity Leave and "Unemployed" have the highest rates of default when compared to their overall percentage. Interestingly, however counts for both categories do not appear on the first bar plot. This indicates that the the count of overall clients in "Maternity Leave" and "Unemployed" is low when compared to other groups like "Working".

Thus, ggplot has left these classes out to accommodate the much larger classes like "Working" and "Commercial Associate". However, even though the number of clients in "Maternity Leave" and "Unemployed" are small, it is interesting that a large portion of the clients still defaulted. For instance, out of all the clients (women in this case) who are classified as maternity leave, more than 50% of the clients defaulted on their loans. Likewise out of all the clients who are unemployed, almost exactly 50% defaulted on their loans. This does seem to align with broader trends where "employment status" and "maternity leave" impact one's ability to do other things. (The amount of clients who fall into "Unemployed" and "Maternity Leave" for Home Credit is small, so it can't be said for certain whether this is a definitive trend. There are only 4 clients in the maternity leave and 18 clients who are unemployed in the data set for the cleaned data. The unclean data had 5 clients in maternity leave and 22 unemployed clients.)

Afterwards, the next 4 categories, "Working", "Commercial Associate", "State Servant" and "Pensioner" all have much lower levels of default while "Student and "Business" have no default at all. There are however a few things that should be noted:

In the first plot, Student and Businessmen do not show up on the first plot which indicates that their count is relatively small compared to the other groups. This is also supported by the table which shows that there are only 16 students and 4 businessmen. Thus, to accommodate larger groups like Working, the values were left out of the count plot. However, in the second plot, it shows that students and businessmen do not have any defaults. This is interesting however the sample size of students and businessmen in this data set is quite small. Thus, this might not be proof a strong association between being a student or a businessman and not defaulting.

Finally the last 3 groups with relatively large counts (State Servant, Working and Pensioners) have similar level of defaults. The default rate for these groups hovers around 8 to 10%. Thus, it seems that Home Credit may already give preference to people who have an employment type. This results in larger observations for these groups and a similar level of default as well. However for the rare occasions when Home Credit does give loans to Maternity Leave or Unemployed Clients, the default rate is higher, despite Home Credit likely doing stricter checks on these clients. (Pre-Existing Stricter Checks would also explain why Maternity Leave and Unemployed represent a small portion of the Income Type Variable.)

### Gender vs Default Status

Is there a relationship between gender and a client's default status? It is well known that gender plays a role in income, career advancement, etc. and several other phenomena. Thus it's possible that gender could have an influence on the client's default status.

```{r}

# Count Visualization
ggplot(data = train_clean, aes(x = CODE_GENDER, fill = as.factor(TARGET))) + #x should be gender
  geom_bar(position = 'dodge') + # side by side bar
  labs(title = "Distribution of Default Status by Gender", # custom title
       x = "Gender", # custom x-axis title
       fill = "Default Status") +  # Change the legend title here
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) +          # Custom labels for the legend
  theme_minimal() + # Assign a theme
  coord_flip() # flip the graph

# Percentage Visualization
ggplot(data = train_clean, aes(x = CODE_GENDER, fill = as.factor(TARGET))) + #x should be gender
  geom_bar(position = 'fill') + # fill each bar with percentage
  labs(title = "Distribution of Default Status by Gender", #custom title
       x = "Gender", # custom x-axis title
       fill = "Default Status", #change the legend title
       y = "Percentage") +  #custom y-axis title
  scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),  # Assign custom colors
                    labels = c("No Default", "Defaulted")) +      # Custom labels for the legend
  scale_y_continuous(labels = function(x) paste0(x * 100, "%")) +  # Convert to percentages
  theme_minimal() + # Assign a theme
  coord_flip() # flip the graph


```

The first plot shows distribution of default status by gender as counts. The second plot shows the same relationship but as a percentage. The second plot indicates that out of all the men in the dataset, approximately 10% have defaulted on their loans. Likewise for all the women in this dataset, approximately 8% of the women in the dataset have defaulted on their loans. This indicates that gender may have an influence on determining default or difficulty in paying the loan.

## Histograms

### Default Status by Credit Scores Source 1

Is there a relationship between the credit scores and a client's default status? Credit Scores are often utilized to predict whether someone will pay back a loan. Although in Home Credit's case a lot of customers might not have credit scores since Home Credit loans to underserved populations. Either way though, credit scores could still be helpful for predicting who can pay a loan back successfully.

```{r}

train_clean |>  # call train clean and avoid calling it in ggplot
     ggplot(mapping = aes(x = EXT_SOURCE_1, fill = TARGET)) + # use Ext_Source_1 to graph plot
     geom_histogram(bins = 10) + # custom bins of 10
     facet_wrap(facets = ~TARGET) + # Create 2 histograms
     labs(title = "Histogram of Default Status by Credit Scores", # title
          x = "External Credit Card Source 1") + # X-axis
      scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) + # Custom labels
  theme_minimal() # Assign a theme

```

The plot shows that there are more customers in each of credit score bins for customers who did not default compared to customers who do default. This seems to suggest that having a credit score in itself is indicative of default or no default. Interestingly, there is a large number of customers who did not default despite having a credit score of zero. This however could be due to the imputation of giving customers a zero if they did not have credit scores.

The logic as previously mentioned in the cleaning data section was that this Credit Card Agency was stricter with giving credit scores compared to the other credit card agencies. Thus, to reflect that strictness, zero was given to any customers who did not have a credit score.

### Default Status by Credit Scores Source 2

Is there a relationship between the credit scores from Source 2 and a client's default status?

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
     ggplot(mapping = aes(x = EXT_SOURCE_2, fill = TARGET)) + # use Ext_Source_2 to graph plot
     geom_histogram(bins = 10) + # custom bins of 10
     facet_wrap(facets = ~TARGET) + # Create 2 histograms
     labs(title = "Histogram of Default Status by Credit Scores", # title
          x = "External Credit Card Source 2") + # X-axis
      scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) + # Custom labels
      theme_minimal() # Assign a theme

```

The plot shows a similar trend to the histogram for External Credit Card Source 1 in that there are more customers in each of credit score bins for customers who do not default compared to defaulting customers. This again seems to suggest that having a credit score in itself is indicative of default or no default. Furthermore this trend is much more pronounced in this plot with "no default" showing much higher counts of credit scores across all the bins when compared to the "defaulted" group.

### Default Status by Credit Scores Source 3

Is there a relationship between the credit scores from Source 2 and a client's default status?

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
     ggplot(mapping = aes(x = EXT_SOURCE_3, fill = TARGET)) + #x-axis should be ext_source_3 
     geom_histogram(bins = 10) + # custom bins of 10
     facet_wrap(facets = ~TARGET) + # Create 2 histograms
     labs(title = "Histogram of Default Status by Credit Scores", # custom title
          x = "External Credit Card Source 3") + # custom x-axis title
      scale_fill_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                    labels = c("No Default", "Defaulted")) + # Custom labels
      theme_minimal() # Assign a theme

```

The plot shows also shows the same trend to the histogram for External Credit Card Source 2 in that there are more customers in each of credit score bins for customers who do not default compared to defaulting customers. This combined with Credit Card Source 1 and 2 further suggests that having a credit score in itself is indicative of default or no default. Furthermore, just like the Credit Card Source 2 histogram, this trend is much more pronounced when compared to Credit Card Source 1. The plot with "no default" showing much higher counts of credit scores across all the bins when compared to the "defaulted" group.

## Scatterplots

### Total Income vs Annuity Amount

Does Total Income have an influence on the Annuity Amount (Loan Amount) Perhaps clients with a higher income can apply for bigger annuity amounts. This in turn could have an impact on being able to pay back loans. Perhaps clients with higher Annuity Amounts default more then clients with lower Annuity Amounts.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = AMT_INCOME_TOTAL, y = AMT_ANNUITY, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Total Income vs Annuity Amount", # custom title
       x = "Total Income", # custom x-axis
       y = "Annuity Amount", # custom y -axis
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme

```

This scatter plot seeks to determine the influence of income on the annuity amount. It however appears that there is no relationship between the variables. There are multiple points in where the annuity amount continues to increase while income remains the same.

There is additionally no strong trends with "default" and "no default" for the scatter plot. The points are randomly scattered and don't suggest a strong pattern.

### Credit Amount vs Annuity Amount

Does Credit Amount have an influence on the Annuity Amount. Perhaps, clients with a higher credit can apply for larger annuities. A visualization could be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = AMT_CREDIT, y = AMT_ANNUITY, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Credit Amount vs Annuity Amount", # custom title
       x = "Credit Amount", # custom x-axis
       y = "Annuity Amount", # custom y-axis
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme

```

The scatter plot shows the influence of Credit Amount on the Annuity Amount which is a positive relationship. This is because the plot shows when Credit Amount increases, the Annuity Amount also increases as well. Furthermore, there seems to be a somewhat weak positive relationship with default and increasing credit amount. As the Credit Amount increases, "Defaulted" also increases which also corresponds to an increase in the Annuity Amount as well. (It should be noted that No Default also increases as well which indicates that default and no default may increase in general when credit amount goes up.)

### Car Age vs Annuity Amount

Does the age of a client's car have any contribution since cars are typically used for collateral in loans? In the case of Annuity Amount, perhaps an older car will result in a lower annuity amount since older cars are worth less. A visualization could be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = OWN_CAR_AGE, y = AMT_ANNUITY, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Car Age vs Annuity Amount", # custom title
       x = "Car Age", # custom x-axis
       y = "Annuity Amount", # custom y-axis
       color = "Repayment Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) + # Custom labels for the legend
  theme_minimal() # Assign a theme
   

```

This scatterplot shows the relationship between Car Age and Annuity Amount. There is no relationship between the two variables. For instance, at age zero for the car, the annuity amount continues to increase. This can also be observed for every car age which indicates that there is no relationship. Additionally, there is no noticeable trend of grouping between "Defaulted" and "No Defaulted". "Defaulted" appears to be randomly scattered.

### Car Age vs Credit Amount

Does the age of a client's car have any contribution since cars are typically used for collateral in loans? Perhaps older cars lead to a lower a credit since cars depreciate over time. Thus, their value as collateral diminishes overtime. A visualization could be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = OWN_CAR_AGE, y = AMT_CREDIT, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Car Age vs Credit Amount", # custom title
       x = "Car Age", # custom x-axis
       y = "Credit Amount", # custom y-axis
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Custom Theme

```

This scatterplot shows the relationship between Car Age and Credit Amount. There is no relationship between the two variables. For instance, at age zero for the car, the credit amount continues to increase. This can also be observed for every car age which indicates that there is no relationship. Additionally, there is no noticeable trend of grouping between "Defaulted" and "No Defaulted". "Defaulted" appears to be randomly scattered.

### Days Since Birth vs Goods Price

Age could also play a role in default. (Days Since Birth represents Age.) Perhaps, younger clients default more because they are just starting out which means they won't have much assets or cash. This also means that the goods they seek to purchase may be smaller when compared to older clients. A visualization can be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = DAYS_BIRTH, y = AMT_GOODS_PRICE  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Days Since Birth vs Goods Price", # custom title
       x = "Days Since Birth", # custom x-axis
       y = "Goods Price", # custom y-axis
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Custom Theme

```

This scatterplot shows the relationship between Car Age and Credit Amount. There is no relationship between the two variables. For instance, at age zero for the car, the credit amount continues to increase. This can also be observed for every car age which indicates that there is no relationship. Additionally, there is no noticeable trend of grouping between "Defaulted" and "No Defaulted". "Defaulted" appears to be randomly scattered.

### Days Since Birth vs Total Income

Age could also play a role in default. Perhaps younger clients default more because they are just starting out which means they won't have much assets or income. A visualization can be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = DAYS_BIRTH, y = AMT_INCOME_TOTAL  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Days Since Birth vs Total Income", # custom title
       x = "Days Since Birth",  # custom x-axis
       y = "Total Income",  # custom y-axis
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Custom Theme

```

This scatterplot shows the relationship between Days Since Birth and Total Income. There however is no relationship between these two variables with the plot being densely scattered. Additionally, "Defaulted" is randomly scattered with no apparent pattern.

### Days Since Birth vs Car Age

Age could also play a role in default. Perhaps younger clients default more because they are just starting out which means they won't have much assets or cash. This also means that the goods they seek to purchase may be smaller when compared to older clients. A visualization can be utilized to explore this relationship.

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = DAYS_BIRTH , y = OWN_CAR_AGE, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Days Since Birth vs Car Age", # custom title
       x = "Days Since Birth", # custom x-axis
       y = "Car Age", # custom y-axis
       color = "Repayment Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Custom Theme

```

This scatterplot shows the relationship between Days Since Birth and Car Age. There is no relationship between the two variables. For instance, at 10,000 Days Since Birth, there is no change in the Car Age. Additionally, there is no noticeable trend or grouping between "Defaulted" and "No Defaulted". "Defaulted" appears to be randomly scattered.

## Highly Correlated Variables

```{r}

# Step 1: Select numeric columns excluding 'TARGET'
numeric_columns <- sapply(train_clean, is.numeric)
train_clean_numeric <- train_clean[, numeric_columns]

# Step 2: Exclude the 'TARGET' column
train_clean_numeric <- subset(train_clean_numeric)

# Step 3: Calculate correlation matrix (use complete observations)
correlation_matrix <- cor(train_clean_numeric, use = "complete.obs")

# Step 4: Convert correlation matrix to a long format
cor_long <- as.data.frame(as.table(correlation_matrix))

# Step 5: Filter out self-correlations (correlations where a variable is correlated with itself)
cor_long <- cor_long[cor_long$Var1 != cor_long$Var2, ]

# Step 6: Remove redundant pairs (keep only one combination of each variable pair)
cor_long <- cor_long[!duplicated(t(apply(cor_long[, 1:2], 1, sort))), ]

# Step 7: Sort by the absolute value of correlations
cor_long <- cor_long[order(abs(cor_long$Freq), decreasing = TRUE), ]

# Step 8: Extract top 10 positive correlations
top_10_positive <- head(cor_long[cor_long$Freq > 0, ], 10)

# Step 9: Extract top 10 negative correlations
top_10_negative <- head(cor_long[cor_long$Freq < 0, ], 10)

# Step 10: View the top 10 positive and top 10 negative correlations
print("Top 10 Positive Correlations:")
print(top_10_positive)

print("Top 10 Negative Correlations:")
print(top_10_negative)
```

Several of the variables that I thought would have strong relationships for the scatterplots show the opposite. Thus, a correlation matrix will be created and the top 10 positive and negative correlations will be selected. Afterwards visualizations will be created to analyze the variables that I believe are the most interesting.

### Credit Amount vs Goods Amount Price

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = AMT_CREDIT , y = AMT_GOODS_PRICE  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Credit Amount vs Goods Amount Price", # custom title
       x = "Credit Amount", # custom x-axis title
       y = "Goods Amount Price", # custom y-axis title
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme

```

This Scatter Plot shows the relationship between Credit Amount and the Goods Amount Price. There is a strong positive relationship between Credit Amount and Goods Amount Price. Additionally both "No Default" and "Defaulted" are increasing as well. There however does not appear to be a distinct area of the scatter plot where the "Default" points and clustered together and the "No Default" points are clustered together.

### Children Count vs Family Count

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = CNT_CHILDREN , y = CNT_FAM_MEMBERS  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Children Count vs Family Member Count", # custom title
       x = "Children Count", # custom x-axis title
       y = "Family Member Count", # custom y-axis title
       color = "Repayment Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme

```

This Scatter Plot shows the relationship between Children Count and the Family Member Count. There is a somewhat positive relationship between Children Count and the Family Member Count. Interestingly only "No Default" is present in the graph. This suggests that perhaps clients who have children mostly don't default.

The table shows that there are a small number of clients who have children and do default on their loans.

### Goods Price Amount vs Annuity Amount

```{r}

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = AMT_GOODS_PRICE , y = AMT_ANNUITY  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Goods Amount Price vs Annuity Amount", # custom title
       x = "Goods Amount Price", # custom x-axis title
       y = "Annuity Amount", # custom y-axis title
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme


```

This scatter plot shows the relationship between the Goods Amount Price and Annuity Amount. There is a positive relationship between these two variables with Annuity Amount increasing as Goods Amount Price goes up. Additionally there does appear to be a cluster of "Defaulted" from 500,000 to 1,000,000 dollars for Goods Amount Price. This seems to suggest that a lot of defaults may occur when the client is attempting to purchase an item that is worth roughly 500k to 1M.

Afterwards, the there is not a really distinct group and the rest of the default observations somewhat increase as the Goods Amount Price increases.

### Client's Region Rating vs Relative Region Population

```{r}
	
train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = REGION_POPULATION_RELATIVE , y = REGION_RATING_CLIENT, color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Client's Region Rating vs Relative Region Population", # custom title
       x = "Client's Region Rating", # custom x-axis title
       y = "Relative Region Population", # custom y-axis title
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme


```

The correlation between these two variables is -.50. It however appears that there may not be any relationship based on this scatterplot. For instance, the Relative Region Population (normalized population of a client's region) has the same score even though the Client's Region Rating changes. (Client's Region Rating represents the score that Home Credit has given to a client's region.)

Additionally there is no distinct grouping or trend between "No Default" and "Defaulted".

### Median Living Area vs Median Living Area

```{r}
	
train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = LIVINGAREA_MEDI , y = APARTMENTS_MEDI  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Median Maximum Floors vs Median Apartment", # custom title
       x = "Median Living Area", # custom x-axis title
       y = "Median Apartment", # custom y-axis title
       color = "Repayment Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme

```

Several of the housing variables like LIVINGAREA_MEDI and APARTMENTS_MEDI are shown to be somewhat correlated at 0.49. It however seems unlikely that all these variables would have a strong effect on predicting default. Thus, only of 2 of the correlated variables will be selected for experimentation and to see if there is any relationship with default.

Thus, only Median Living Are and Median Apartment have been selected to see if there is a relationship due to relatively high correlation at .49. ( I am not selecting FLOORSMAX_MEDI/ELEVATORS_MEDI which has a correlation of 0.5134187. This is because I believe that most clients are unlikely to have elevators in their houses which makes this variable combination unrepresentative of the population.)

Finally, for the overall plot, it appears that there is somewhat moderate positive relationship with the Median Living Area associated with an increase in Median Apartments. (This mainly occurs when Median Living Area is approximately .50)

Default also seems to have a very weak cluster from a Median Living Area of 0 to 0.25. Beyond that, defaulted appears to be randomly scattered.

### Default In 60 Days Social Circle vs Default In 30 Days Social Circle

```{r}
	

train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = DEF_60_CNT_SOCIAL_CIRCLE , y = DEF_30_CNT_SOCIAL_CIRCLE  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Default In 60 Days Social Circle vs Default In 30 Days Social Circle", # custom title
       x = "Default In 60 Days Social Circle", # custom x-axis title
       y = "Default In 30 Days Social Circle", # custom y-axis title
       color = "Default Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme


```

This is a plot that compares the relationship between Default in 60 days (How many people in the Client's social circle defaulted on their loans within 60 days.) and Default in 30 days. (How many people in the Client's social circle defaulted on their loans within 30 days.)

There does appear to be a positive relationship between the two variables with an increase in Defaulting in 60 days corresponding to an increase in defaulting in 30 days. No Default follows this pattern, however Defaulted seems to have no strong pattern. For instance, there are 0 clients who defaulted in 60 days but 2 clients who did default in 30 days.

```{r}
	
train_clean |> # call train clean and avoid calling it in ggplot
  ggplot(mapping = aes(x = OBS_60_CNT_SOCIAL_CIRCLE , y = OBS_30_CNT_SOCIAL_CIRCLE  , color = as.factor(TARGET))) +  # Map TARGET to color
  geom_point() +  # Scatter plot
  labs(title = "Scatter Plot of Late Pmt In 60 Days vs Late Pmt In 30 Days", # custom title
       x = "Late Payments In 60 Days Social Circle", # custom x-axis title
       y = "Late Payments In 30 Days Social Circle", # custom y-axis title
       color = "Repayment Status") +  # Add a label for the color legend
  scale_color_manual(values = c("0" = "#40DE98", "1" = "#DE5140"),   # Assign custom colors
                     labels = c("No Default", "Defaulted")) +  # Custom labels for the legend
  theme_minimal() # Assign a theme


```

This scatterplot shows a similar relationship between late payments (60 days past due) and late payments (30 days past due). The relationship is positive. There however does not seem to be a strong pattern for defaulted. For instance, even though late payments (60 days) changes from 0 to 1.25, late payment still remains the same at 1.25. This can be seen for other observations as well such as 2.3 for late payments (60 days) where late payment 60 remains at 2.3 while late payment(30 days) increases from 3.75 to 3.8

# Results

The EDA was very interesting and revealed a lot of things. For instance, I thought that some variables would have strong relationships but the relationship was not very strong. Here are some of those variables:

-   Birth vs Car Age
-   Birth vs Total Income
-   Birth vs Goods Price
-   Total Income vs Annuity Amount
-   Martial Status

I was particularly surprised that there was not a strong relationship between Total Income and Annuity Amount. Martial Status also surprised me as well.

There were other variables that did have an influence and provided some insights into potential causes of default.

-   Credit Amount vs Goods Price
-   Goods Amount Price vs Annuity Amount
-   The External Credit Scores
-   Gender
-   Income
-   Education

I believe that it should now be possible to build a model that is parsimonious using the above predictors to better predict default. Additionally there were several missing values that had to be addressed carefully with imputation depending on the context. Some columns like Car Age were imputed to zero because it meant that the owner did not own a car while other columns like the external credit scores used values from both columns to make educated guesses about the values. Moreover low variance columns also needed to be addressed, since they don't offer any predictive value and may slow down modelling.

Finally, after completing the EDA, it is very apparent that most clients pay their loans since 0 represents successful loan payback which is 91% of the data. This can also be seen in indirectly in every scatterplot visualization. Thus, a better metric like "Precision" or "F1 Score" should be used in place of Accuracy to gain a more nuanced insight into Home Credit's operations and to evaluate any new models.
