---
title: "Case Template"
author: "Varun Selvam"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
execute:
  include: true
  eval: true    
  warning: false
  message: false
self-contained: true
editor: 
  markdown: 
    wrap: 72
---

# Load Libraries

```{r}

pacman::p_load(tidyverse,e1071,caret,rminer,rpart,C50,tictoc,kernlab,snow,parallel,doParallel,randomForest,Matrix)

```

# Import Dataset

```{r}

cleaned_dataset <- read_csv("C:/Users/User/Box Sync/Business Analytics Degree/Semesters/Fall Semester 2024/IS 6812/EDA/Official_EDA/Train_Test_Files/train_clean.csv")

```

## Clean the Dataset

There were some changes that I did to the dataset like converting certain numeric and character columns to factors. However, when the file was exported to a csv after completing the EDA, that information was lost. Thus, to deal with that issue, some of the columns must be reconverted to factors.

```{r}

# Mutate any character variables into factors
cleaned_dataset <- cleaned_dataset %>% mutate_if(is.character, as.factor)

# Numeric Variables which should be factors stored as a vector
cat_values <- c("FLAG_EMP_PHONE","FLAG_WORK_PHONE","FLAG_EMAIL","FLAG_PHONE",'REG_REGION_NOT_WORK_REGION','LIVE_CITY_NOT_WORK_CITY',"REG_CITY_NOT_LIVE_CITY","REG_CITY_NOT_WORK_CITY","REGION_RATING_CLIENT_W_CITY","FLAG_DOCUMENT_3","FLAG_DOCUMENT_6","FLAG_DOCUMENT_8","REGION_RATING_CLIENT")

# Utilize mutate to transform all the columns in the cat_values vector to factors.
cleaned_dataset <- cleaned_dataset %>% 
  mutate(across(all_of(cat_values), as.factor))  

# Display the summary of train_clean
str(cleaned_dataset)


```

The code has read in the file and is now displaying some information such as whether it's a character, factor, etc. for each variable.

# Build Majority Classifier

A majority classifier just uses the majority of a class to make predictions about the entire dataset. For instance, in Home Credit's Case, `Target` is a binary variable which can take two values: 

Default - 0 
No Default - 1.

If the majority of applicants in this dataset do not default, then the majority class is "No Default". The majority classifier will then predict "No Default" for everyone in the dataset.

## Check Class Prevalence

```{r}

# Display counts for the target variable and convert to a proportion.
prop.table(table(cleaned_dataset$TARGET)) %>% round(2) # Round to 2 decimal places.

```

The proportion of 0 and 1 for the cleaned dataset is still the same. 0 represents `No Default` and while 1 represents `Default`. This means that the majority of clients in this dataset did not default.

## Get Counts of the Variables

```{r}

# Display counts for each level of the "TARGET" Variable
table(cleaned_dataset$TARGET)

```

This just displays the counts for the target variable. There are 218,531 customers who did not default and there are 19,245 customers who did default.

## Code Majority Classifier

```{r}

# No and Yes Count
yes_count <- 218531
no_count <- 19245

# Calculate the total number of observations
total_count <- yes_count + no_count

# Majority class
majority_class <- ifelse(yes_count > no_count, 0, 1) # 1 if yes_count is greater than no count

# Convert Majority Class to factor
majority_class <- factor(majority_class)

#Generate the majority classifier predictions for all instances
predicted <- rep(majority_class, total_count) # Total count represents the total number of instances
# since majority class is zero, this will be repeated for every prediction or total number of instances.

# Create metrics list for Majority Classifier  
metrics_list = c("ACC","TPR","PRECISION", "F1", "CONF", "AUC", "ROC")

# Generate metrics
majority_output <- mmetric(factor(cleaned_dataset$TARGET), predicted, metrics_list) #Set all the metrics generated by the mmetric function to an output.

majority_output_rounded <- lapply(majority_output, function(x) { # Utilize function to round off metrics to 2 decimal places.
  if (is.numeric(x)) {
    return(round(x, 2))
  } else {
    return(x)
  }
})

print(majority_output_rounded)

```

TPR stands for Recall

The above code shows that the accuracy is 91.92% which rounds to 92%. The reason for this is because the majority classifier predicts the majority class which is "No Default".

The majority classifier has an overall accuracy of 91.93%. This is however because the majority class is 0 which is "No Default". (Majority Classifiers predict the majority class for everything which means that accuracy will be the majority class value.)

However since the objective is to predict the clients who can pay back their loans, the majority class accuracy can be used as the baseline accuracy. This also means that any future model will have to beat this majority class accuracy. (91.93%)

Although the accuracy of the majority model is interesting as a baseline metric, it might not be very useful due to the high class imbalance.

Thus, some additional metrics that may be desirable to be beat is `Precision 1`, `F1-1` or `ROC-AUC` .

It should be noted that beating the recall for 1(pay back loan) will not be possible since it is 100%. This is because TPR1 represents the majority class and the majority classifier will make no false negative errors. Thus, everything will be classified as positive which divided by the total positive observations results in 100.

ROC-AUC however is a measure of how well the model makes predictions in regards to different classes of the target variable. (This is measured by comparing the True Positive to the True Negative Rate.) This might be a more useful metric due to the imbalance and will reveal more interesting insights about the model.

-   True Positive represents correctly predicting that somebody will be
    able to pack a loan.
-   True Negative represents correctly predicting that somebody will
    have difficulty paying back a loan.
-   False Negative represents incorrectly predicting difficulty paying
    back the loan.
-   False Positive represents incorrectly predicting that somebody will
    pay back the loan when they actually will have difficulty paying
    back the loan.

# Build Random Classifier

```{r}

class_labels = c(0,1) # Create a class label with 2 labels, 0 and 1
class_labels <- factor(class_labels) # Convert Class labels to a factor

set.seed(123) # Set seed for reproducibility
predictions <- sample(x = class_labels, size = total_count,replace = TRUE, prob = c(.5,.5)) # Sample from the class label with replacement. Each observation has a 50% chance of being selected.

predictions <- factor(predictions) # Convert the predictions into a factor

random_output <- mmetric(factor(cleaned_dataset$TARGET),predictions,metrics_list) # Generate metrics


random_output_rounded <- lapply(random_output, function(x) { # Utilize function to round off metrics to 2 decimal places.
  if (is.numeric(x)) {
    return(round(x, 0))
  } else {
    return(x)
  }
})
print(random_output_rounded) # Print out the rounded metrics

```

The Random Classifier model has an overall accuracy of 50% due to randomly assigning an observation to either default or no default with 50% probability. 

Consequently, this has also affected recall which
measures the following: predicted class/total observations in the actual class

The objective of any model built will be to have an accuracy higher than 50% and to have a higher recall (greater than TPR1), higher precision1, and a higher F11 score as well.

# Partition the Dataset

The below code partitions the dataset into a train and test set with 80% of the data in the train set and 20% of the data in the test set.

This is an important step, because it is possible that models can over fit. In other words, the model may learn noise in the data as opposed to actual patterns. Consequently, when the model sees new data, it won't make accurate predictions and performance will instead plummet.

Thus, to avoid this, cross-validation can be utilized where the model is split into 80% train and 20% test. The 20% test will be used to test the model to ensure that the model is actually learning the patterns and not overfitting.

```{r}

# Make Zero the level to be modelled aka the positive class
cleaned_dataset$TARGET <- factor(cleaned_dataset$TARGET, levels = c("1", "0"))  # Set 0 as the positive class

cleaned_dataset <- cleaned_dataset %>% 
  select(-SK_ID_CURR)

set.seed(123)
row_indexes <- createDataPartition(cleaned_dataset$TARGET,p=.7,list = FALSE) # This splits the dataset into train and test, while maintaining the class balance of the "TARGET" Variable.

train_set <- cleaned_dataset[row_indexes,] # subset 70% of rows from cleaned dataset to the train set.
test_set <- cleaned_dataset[-row_indexes,] # take all remaining rows which is 30% and move to the test set.

```

The Target Variable has been changed with Target becoming a factor and 0 becoming the second level. This is because R always models the second level of a factor by default.

Additionally, it should be noted that 0 - `No Default` is considered the positive class while 1 - `Default` is considered the negative class.

# Decision Tree

## Train Decision Tree

We'll first build a simple decision tree model using all the predictors and the default hyper-parameters.

```{r}

tic() # See how long decision tree takes to model
tree_model <- rpart(formula = TARGET ~ ., data=train_set) # Create tree model using the rpart algorithm. Utilize all predictors
toc() # See how long decision tree takes to model

```

This decision tree trained relatively quickly at 7.55 seconds.

## Display Tree Model

```{r}

tree_model # Display the tree model

summary(tree_model) # Important information about the tree model
```

This decision tree is very simple, with no splits made due to the high class imbalance (92% positive class - No Default). Decision trees typically create splits to group similar observations, aiming to separate classes effectively. However, the imbalance in this dataset made it challenging to find meaningful splits.

While decision trees are powerful, they can overfit if the splits become too precise, capturing noise rather than meaningful patterns.

## Compute Evaluation Metrics

The below section will evaluate how this decision tree performs on various metrics for the train and test sets. The most important metric however is AUC-ROC.

### Evaluation Train Metrics

Evaluate Decision Tree Performance on the train set.

```{r}

tree_model_train_pred <- predict(tree_model,newdata = train_set) # Make predictions with the tree model on the train dataset

mmetric(train_set$TARGET,tree_model_train_pred,metrics_list) # Generate predictions with the tree model on the train dataset.

```

The decision tree has a slightly lower accuracy then a majority classifier at 91.91%. The AUC-ROC is however not great at 0.5. A score of 0.5 indicates that the decision's tree ability to distinguish between the two classes (default and no default) is as good as random guessing.

The model's performance however on the minority class is not great with 0% on the minority F1 Score, Precision and Recall. In terms of recall, this means that the model failed to identify any instance of the negative class. This also adversely affects precision because if the model failed to identify any negative class, then one can't correctly identify how much of the model's predictions are negative.

The model however performs fairly well with the positive class being able to identify correctly identify all the positive instances (successfully pay loan) and having a precision of 91.91%. This means that out all the identified positive instances, 95.78% were indeed actually positive. The positive F1 score is also great which indicates that the model can balance between precision and recall for the positive class. (In the context of Home Credit, the model is able to identify all instances of successful loan payback and in instances where it has identified loan payback, it has done so successfully at 91.91%.)

Thus, this model is really good at identifying those who can pay back their loans, since they are the majority class. It however is not great at identifying defaulters since they are the minority class. (It would seem this tree model is almost exactly like a majority classifier since it's performance with Accuracy and Recall is identical. Accuracy is slightly lower at 91.91% vs 92% for a majority classifier.)

### Evaluation Test Metrics

```{r}

tree_model_test_pred <- predict(tree_model,newdata = test_set) # Make predictions with tree model on test dataset

mmetric(test_set$TARGET,tree_model_test_pred,metrics_list) # Generate metrics with tree model predictions on the test datset

```

The model maintains the same performance as the test set with an accuracy of 91.92%. All of the other metrics are the same for both the positive and negative class as well.

Overall, this decision tree generalizes well to new data since it maintains the same performance on the train set as well. It however also has the same issues like being unable to effectively distinguish between the majority class (no default) and minority class (default). The metrics are also similarly not very great for defaulters. The model struggles broadly based on precision, recall and F1 to identify defaulters.

# Decision Tree with Weights

A potential solution to deal with this class imbalance is to assign a greater weight to the minority class. We'll use weight implementation with a Decision Tree.

## Create Weights

```{r}

# Set class weights (example: 10x weight for the minority class)
class_weights <- ifelse(cleaned_dataset$TARGET == 1, 10, 1)

```

Assign the weights to the class_weights variable. For minority, a weight of 10 is assigned. This means that the observations in the minority class will be considered 10 time more important than observations in the majority class.

It will also force the model to do splits that maximize the separation of the minority class due to the higher weights.

## Create Model with Weights

```{r}
# Fit rpart model with class weights
tic() # See how long model takes to train
tree_model_w <- rpart(TARGET ~ ., data = cleaned_dataset, weights = class_weights, method = "class")
toc() # See how long model takes to train

```

This model was relatively quick to train.

## Summary of Weighted Model

```{r}

tree_model_w
summary(tree_model_w)

```

The weighted decision tree has performed much better with 12 splits in contrast to the un-weighted decision tree. Additionally, we can see that the top 3 important predictors for predicting successful loan repayment were `EXT_SOURCE_2`, `EXT_SOURCE_3` and `OCCUPATION_TYPE`.

## Compute Evaluation Metrics

The weighted decision tree will now be cross-validated with the train and test sets.

### Evaluation Train Metrics

```{r}

tree_model_w_train <- predict(tree_model_w,newdata = train_set) # Make predictions for weighted tree model on the train dataset.
mmetric(train_set$TARGET,tree_model_w_train,metrics_list) #Generate Metrics to analyze weighted tree model performance on the test dataset.

```

The weighted decision tree does much better on the ROC-AUC metric at 0.68. This is much better than the regular Decision Tree which had a metric of 0.50. This means that model is doing a much better job at discriminating between the positive class (no default) and negative class (default).

ROC-AUC can go up to 1, so this model is doing relatively well on this metric. It also does better on all the metrics for the minority class as well. For instance, this model is able to identify 53.59% of all negative instances (default). This is significant progress since the un-weighted decision tree was at 0% for this recall.

Additionally, for precision, the minority class metric is at 16%. This much better then the un-weighted decision tree which was at 0%. In the context of Home Credit, this means that the model out of all the negative instances that weighted tree identified, 16% were actually negative. This in some ways is more concerning then accuracy because it costs more money (median of 24,412 dollars when someone defaults) compared to rejecting someone who could pay back the loan. (median of 23,800 dollars for incorrect rejections)

The F1 Score is much better when compared to the un-weighted decision tree model for the negative class at 24.63%. Although better than the un-weighted decision tree, it could still be better at striking a balance between the precision ad recall. 24.63% is a bit low.

The positive class (No Default) for Precision and Accuracy has a decline of approximately 23 to 25 percent. All the other metrics have stayed the same. This indicates that the weights enabled the decision tree to better predict some of the negative class. It however has reduced the performance with identifying the positive class. The precision for the un-weighted decision tree was 100% which meant that out of all the positive instances predicted by the model, all of them were correct. However, in this case, it decreased to 75.2%. This means that out all the positive predicted instances, 75.2% of the positive predicted instances were correct.

-   The overall accuracy which represents overall model predictions also decreased to 75.2% from 91.92%. This means that the overall ability to correctly predict predictions has gone down. This however is not a great metric to gauge model performance due to high class imbalance. (Correctly predicting the majority class alone would give a very high accuracy since it comprises 91% of the dataset. Hence, although this is concerning, it's not very important when compared to the decline in other models.)

### Evaluation Test Metrics

```{r}
# Make predictions with the Weighted Tree Model on the test set.
tree_model_w_test <- predict(tree_model_w,newdata = test_set)
mmetric(test_set$TARGET,tree_model_w_test,metrics_list) # Generate predictions on the test set to gauge model performance

```

The metrics from the test set are basically the same as the train set metrics. This means that the model has low variance because it's able to generalize well to new data and maintain the same performance that it maintained on the train set. It however has high bias since the metrics for the minority class are very low. (The minority class metrics are the same when compared to the train set, but it's still low overall).

Thus this is a low variance model since it cannot adequately predict the minority class, but it does this consistently for both the train and test sets. It however has high bias because as previously mentioned, the performance is not great on any of the metrics for the minority class. (Model does however perform better than an un-weighted decision tree.)

# Naive_Bayes

The next model that will be examined is Naive Bayes, where the Bayes Theorem is used to classify instances.

## Build Model

The code chunk down below trains a Naive Bayes model on the train set.

```{r}

# Build the Naive Bayes Model utilizing the e1701 package.
tic() # See how long it takes model to train
nb_model <- naiveBayes(TARGET ~ ., data = train_set) # train the model on the train dataset.
toc() # See how long it takes model to train\

```

This model was very quick to train with a total training time of 1.54 seconds.

## Summary of Naive_Bayes Model

The below code will display more information about the Naive Bayes Model.

```{r}

nb_model # Display naive bayes model
summary(nb_model) # Display a summary/key highlights from the model

```

The apriori probablities which can be thought of as "inital inferences" or guesses represents the probability of selecting each class based on their proportion before looking at any of the other data. In this case the "Apriori" probabilities are 8.09% for class 0 and 91.91% for class 1.

-   Class 0 represents no default
-   Class 1 represents default

The model has then calculated the conditional probabilities for each
column. i.e $$p(0) = p(ExtSource1|target = 0) *
p(ExtSource2|target = 0)* .... p(target = 0)$$

The p(target = 0) represents the apriori probability.

Since the apriori probability is simply the proportion of target variable's levels this means that no default's apriori probability will be 91.91% while default's apriori probability is 8.09%.

The probability can also be calculated for the probability of no default as well:

i.e $$p(1) = p(ExtSource1|target = 1) * p(ExtSource2
 |target = 1) * ... p(target = 1)$$

Afterwards, whichever probability is the greater for each observation will be used to make the prediction. For instance, if the probability of "No Default" is .78 and the probability of "Default" is .45, the algorithm will classify the observation as "No Default".

## Make Predictions

```{r}

tic() # Keep track of how long it took to make predictions
# Generate predictions on the train set using the naive bayes model
nb_train <- predict(nb_model,newdata = train_set,type = "raw") 
# Generate predictions on the test set using the naive bayes model
nb_test <- predict(nb_model,newdata = test_set, type = "raw")
toc()

```

## Compute Evaluation Metrics

This section contains the train and test set metrics for the Naive Bayes Model.

### Evaluation Train Metrics

```{r}
# Generate metrics for the model's train predictions
mmetric(train_set$TARGET,nb_train,metrics_list)

```

The model seems to perform better than the regular Decision Tree.

The model did slightly better at discriminating between the two classes (default and no default) with an AUC of 0.6924. The weighted decision tree had an AUC of 0.6788.

In the context of Home Credit, this means that the Naive Bayes model is able to better distinguish between defaulting clients and non-defaulting clients in the dataset.

The Naive Bayes Model also performed better in some of the minority class metrics when compared to a decision tree. It should however be noted that the decision tree's performance was zero for the minority class. This means that any model that delivers non-zero results for the minority class will do better like the Weighted Decision Tree.

The Naive Bayes model does better in terms of precision and F1 Score when compared to the Weighted Decision Tree. The Weighted Decision Tree's precision score was 16% while Naive Baye's precision score was 19.62%. The F1 Score was also slightly better for Naive Bayes at 25.70% vs 24.63% for the Weighted Decision Tree.

This means that the Naive Bayes model is able better able to balance recall and precision better than the Weighted Decision Tree for the negative class. The model's performance however for the positive class is better for recall and F1 score as well. For instance, the weighted decision tree's recall score was 75% while the Naive Bayes's score was 86.56%. The F1 score was also better at 90.13% for Naive Bayes compared to 83.75% for the Weighted Decision Tree. Naive Bayes does however come very close to the Weighted Decision Tree for the positive precision score as well. (Weighted Decision Tree Precision Score was 94.82% vs 94% for Naive Bayes Model.)

Accuracy is also higher overall, but as previously mentioned, not a very great metric due to the high class imbalance.

Hence, based on the train set, the Naive Bayes model seems comparable to Weighted Decision Tree. It also performs slightly better then Weighted decision tree as well. The model however needs to be tested on the Test Set to fully gauge performance.

### Evaluation Test Metrics

```{r}
# Generate metrics for the model's test predictions
mmetric(test_set$TARGET,nb_test,metrics_list)

```

This model's test set metrics are very comparable to the train set metrics. Thus, this model is generalizing well to new data. This also indicates that the model is low on variance but moderately high on bias since some of the metrics for the negative class are low like precision which is at 19.41% or recall which is 36.64%.

Overall this model does not perform better then the Weighted Decision Tree which had more balanced metrics for the negative class. For instance, the Weighted Decision Tree's metrics vs the Naive Bayes Model are:

Recall (Negative): 

* 53.44 - Weighted Decision Tree 
* **36.64 - Naive Bayes Model**

Precision (Negative): 

* 15.84 - Weighted Decision Tree 
* **19.41 - Naive Bayes Model**

F1 (Negative): 

* 24.44 - Weighted Decision Tree 
* **25.37 - Naive Bayes Model**

AUC: 

* 0.6787 - Weighted Decision Tree 
* **0.6922 - Naive Bayes Model**

This Naive Bayes Model does have a slightly higher AUC score which indicates that it's better at discriminating between the positive and negative classes. However the Weighted Decision Tree has a much higher Recall at 53.44 vs 36.44 for the recall. Moreover all of the Weighted Decision Tree's metrics are slightly lower for Precision and F1.

Thus, the Weighted Decision Tree is still better since it does better in Recall and only slightly worse in Precision and F1.

# Logistic Regression

This is the next model that will be built to predict default and no default.

## Build Logistic Regression Model

```{r}
tic() # Track how long model takes to train
logistic_model <- glm(TARGET ~ .  -House_Attribute_Low_Variance, data = train_set, family = binomial) # Train model, remove House_Attribute_Low_Variance, since it causes problems in the model
toc() # Track how long model takes to train
```

This model took 1.34 minutes to train

## Summary of Logistic Regression Model

```{r}
# Get a key highlight of this model
summary(logistic_model)

```

This shows key information for the Logistic Regression model. The information however can be a lot, so I have isolated the coefficients. Afterwards I have organized it in descending order.

### Order Coefficients in Descending Order

```{r}
# Extract the coefficients from the logistic regression model
log_coefficients <- logistic_model[["coefficients"]]

# Reordering the coefficients in descending order with sort function
sorted_coefficients <- sort(log_coefficients, decreasing = TRUE)

# View the sorted coefficients
print(sorted_coefficients)

```

`NAME_FAMILY_STATUSUnknown` and `Ext_Source_2` and `Ext_Source_3` are shown as important predictors in successful repayment. The coefficients are in log-odds which makes the exact interpretation difficult. However, since these coefficients are bigger then the other coefficients, we can tell that these predictors have the most influence. Moreover, the sign is positive which indicates that a one unit increase in these predictors increases the log-odds of successfully repaying a loan. These results are also statistically significant at a significance level of .05.

`NAME_FAMILY_STATUSUnknown` however is a factor but for anyone in the "Unknown" category, the log-odds of repayment increase. This does seem a little strange, one would think that one with a known family status might have a better chance of paying back a loan. It should be noted that Home Credit's clients do not have conventional financial histories. Thus, it's possible that an unknown family status could be indicative.

Overall though, 2 out of the 3 top predictors also match with the Weighted Decision Tree. `EXT_SOURCE_1` and `EXT_SOURCE_2` match in both Weighted Decision Tree and the Logistic Regression Model. (Weighted Decision Tree however predicted that `Occupation_Type` is the other important predictor while Logistic Regression is predicting "Unknown" Family Status.)

## Make Predictions

Generate predictions for the train and test sets. It should be noted that Logistic Regression produces probabilities as predictions. Thus, a threshold has to be selected to convert the probabilities to class labels.

In the context, of Homecredit, there are 2 values in the `Target` Variable, "0" or "1". The Target Variable has already been converted to a factor with "0" as the second level. Thus, no changes are required for the Target Variable. However, a threshold must still be decided will be 0.9. This is because the classes are severely imbalanced which means that the model will be able to better predict the majority class. Thus, to mitigate this issue, a very high threshold of 0.9 will be selected. This will force the model to only classify someone as no default if their probability of paying the loan off is very high.

```{r}

# Convert probabilities to binary class predictions (0 or 1)
threshold <- 0.9

# Train Predictions
lm_train_probs <- predict(logistic_model, newdata = train_set, type = "response") # Use Response to get the predictions as probabilities for Logistic Regression
lm_train <- factor(ifelse(lm_train_probs > threshold, "0", "1"), levels = c("1", "0")) # Convert the probablities to class labels

# Test Predictions
lm_test_probs <- predict(logistic_model, newdata = test_set, type = "response") # Use Response to get the predictions as probabilities for Logistic Regression
lm_test <- factor(ifelse(lm_test_probs > threshold, "0", "1"), levels = c("1", "0")) # Convert the probablities to class labels


```

## Evaluate Model

Metrics will now be computed to evaluate the model's performance on the train and test metrics.

### Evaluation Train Metrics

```{r}

# Train Metrics
mmetric(train_set$TARGET, factor(lm_train,levels = c(1,0)), 
metrics_list)


```

The Logistic Regression Model's performance is comparable to the other models. For instance, it's Recall for the negative class is better at 58% vs the Weighted Decision Tree's recall which is only 53.44%. This is 4.41% better then the Weighted Decision Tree. Additionally, the precision (18.09%) and F1 (27.58%) scores are also higher when compared to a Weighted Decision Tree, (15.84%) and (24.44%) respectively.

Like the other models it does better on the positive class with fairly high metrics such as 95.41% for Precision. It also seems to do better then Naive Bayes. The model still however needs to be cross-validated to make sure that its performance is consistent.

### Evaluation Test Metrics

```{r}

# Test Metrics
mmetric(factor(test_set$TARGET,levels = c("1","0")),factor(lm_test,levels = c("1","0")),metrics_list)

```

The Logistic Regression's model's performance on the test set is fairly consistent with its train set performance. There is a very minor drop in all the metrics when train and test sets are compared. This also means that the interpretations about the Weighted Decision Tree still hold. Since Logistic Regression's performance against the Weighted DecisionTree has been compared, we'll now compare this model's performance to Naive Bayes.

Logistic Regression does better in terms of Recall, and F1 score and is only slightly lower then Naive Bayes in terms of precision.

Recall (Negative): 

* 36.64 - Naive Bayes Model 
* **57.51 - Logistic Regression**

F1 (Negative): 

* 27.31 - Naive Bayes Model 
* **24.44 - Logistic Regression**

Precision (Negative): 
* 17.91 - Weighted Decision Tree 
* **19.41 - Naive Bayes Model** (Slightly higher then the Weighted Decision Tree)

Thus, Logistic Regression seems than Naive Bayes since it's performance with the positive class is comparable. Additionally it does better in all of the metrics for the negative class except for precision. This however is by a relatively low amount (1.5%)

# Random Forest

Random Forest is an ensemble method that uses several decision trees to make classification predictions. It however is very computationally intensive and is prone to overfitting. Thus, to mitigate this issue, I will model a very simple Random Forest, since very complicated Random Forest Models overfit.

## Pre Process the Data for Random Forest

The train dataset will be converted into a matrix and then all of the variables will be placed into "x" which is all the predictors. "y" will represent the "TARGET" variable.

```{r}

# Convert categorical variables to dummy variables
train_data_clean <- model.matrix(TARGET ~ . - 1, data = cleaned_dataset) # Remove the target variable from the dataset.

# Set up independent and dependent variables
x <- as.data.frame(as.matrix(train_data_clean)) 
y <- as.factor(cleaned_dataset$TARGET) 

# Reorder the levels in "y" to make "0" the second level
y <- factor(y,levels = c(1,0))

```

### Partition the Dataset

The dataset will be partitioned into an 80/20 split. The train dataset will first be converted into a matrix and then partitioned with 80% of the data being used for training and 20% used for testing.

```{r}
# Subset data for testing
set.seed(123) # Set seed for reproducibility

# Sample 20% of the data and assign to sample_index
sample_index <- sample(1:nrow(x), size = 0.2 * nrow(x))  

# train set w/ all predictors and no target variable
x_sub_train <- x[sample_index, ] # Subset all predictors to x_sub
# target variable from test set
y_sub_train <- y[sample_index] # Subset target variable to y_sub

# test set w/ all predictors and no target variable
x_sub_test <- x[-sample_index,] # Subset all predictors to x_sub_test
y_sub_test <- y[-sample_index] # Subset all predictors to y_sub_test

```

The dataset has now been sucessfully partitioned into train and test sets with 80% in the train set and 20% in the test set.

## Train the Model

```{r}

tic()
set.seed(123)
# Train Random Forest on subset with reduced trees and features
rf_model <- randomForest(x_sub_train, y_sub_train, ntree = 50, mtry = log(ncol(x)), importance = TRUE)
toc()

```

This model took 5.29 minutes to train which is relatively fast for a Random Forest. (This is however a farily simple model.)

## Model Information

### Model Summary

```{r}

print(rf_model) # Print model summary

```

The overall out-of-bag (OOB) error is approximately 8%, which means the model misclassified around 8% of the observations. OOB error is an internal estimate of test error in Random Forests.

This code will extract the top 15 most important features from the XGboost Model. Thet model however has a very high error rate with the minority class (99.87%). In contrast, the majority class has a lower error rate that is less than 1%. This indicates that the class imbalance is affecting the model because Random Forest is only picking up the patterns for the majority class.

However to a get a full understanding, we'll still cross-validate Random Forest with the train and test sets.

### Extract Important Features

```{r}

# Extract feature importance and select top 15 features for better visualization
importance_df <- as.data.frame(importance(rf_model))
importance_df$Feature <- rownames(importance_df)
top_features <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  top_n(15, MeanDecreaseGini)

# Plot top 15 feature importance using ggplot2
ggplot(top_features, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip coordinates for horizontal bar chart
  labs(title = "Top 15 Features in Random Forest Model",
       x = "Features",
       y = "Mean Decrease in Gini") +
  theme_minimal()

```

This plot is based on the Gini Index which measures impurity or the probability of something being erroneously categorized. A predictor with a lower Gini Index indicates that the feature does a good job at reducing "uncertainty".

The top 3 important features are `EXT_SOURCE_2`, `EXT_SOURCE_3` and `DAYS_BIRTH`. These predictors also very important for reducing impurity in the model as well.

`EXT_SOURCE_2` and `EXT_SOURCE_3` match with the other models.`DAYS_BIRTH` however is different and does not match up with the other models. For instance, the 3rd predictor for the Weighted Decision Tree was `OCCUPATION_TYPE`. The third predictor in Logistic Regression was`NAME_FAMILY_STATUSUnknown`

This indicates that at two of the top most important predictors are `EXT_SOURCE_2` and `EXT_SOURCE_3` since they appear in every model.

# Generate Predictions

The code down below will generate predictions utilizing the Random Forest Model on the train and test sets. It will also help us get a full understanding of Random Forest's performance. Random Forest's model summary indicated that the results are not great for the minority class. Cross-Validation will however help us confirm or repudiate those results.

```{r}

# Generate for the training data
predictions_train <- predict(rf_model, newdata = x_sub_train)

# Make predictions on the tesing data
predictions_test <- predict(rf_model, newdata = x_sub_test)

```

Predictions have been sucessfully generated for the train and test sets.

## Generate Train Metrics

This code generates the metrics for the model's performance on the train set.

```{r}

# Train Metrics
mmetric(y_sub_train,predictions_train,metric = metrics_list)

```

The model seems to do very well with high metrics for the positive class. classes. This model's metrics are actually higher than the rest of the models for both classes. The accuracy is even higher as well by 4.71%. (The majority classifier had an accuracy of 91.92%.)

These are the negative class metrics for the other 3 models:

Recall (Negative): 

* 57.51 - Logistic Regression 
* 36.64 - Naive Bayes Model
* **57.81 - Random Forest**

F1 (Negative): 

* 24.44 - Weighted Decision Tree 
* 25.37 - Naive Bayes Model
* **100 - Random Forest** (Much Higher)

Precision (Negative): 
* 17.91 - Weighted Decision Tree 
* 19.41 - Naive Bayes Model 
* **73.27 - Random Forest** (Much Higher)

The above metrics show that Random Forest is doing much better with the negative class when compared to the other models. This is spite of the high error rate for the minority class from the model summary.

However this great model performance must be cross-validated to ensure that the model is not overfitting.

## Generate Test Metrics

This code generates the metrics for the model's performance on the test set.

```{r}

# Test Metrics
mmetric(y_sub_test,predictions_test,metric = metrics_list)


```

The model maintains comparable performance, however it over-fits very severely for the negative class (default). For instance, Recall is 0.0064%, and F1 Score is 0.01295%. This is nearly a 100% drop in all of the metrics. The precision is 100% for the negative class, however this is not very informative. (Precision is only 100% because the model made only 1 prediction of a negative instance and it got it right. Precision is a measure of how many instances are actually negative out of all the predicted instances. So if the model makes one predicted instance and gets it right, then the precision will be 100%.)

This indicates that the Random Forest will need more hyper-parameter tuning to increase generalization to new data. I made the model fairly simple to avoid overfitting, but this does not seem to be very effective. It would appear that other parameters like regularization will need to be included, to improve Random Forest's performance.

Finally, it appears that the model's summary was correct, which cross validation has further proved. The model struggles to classify the minority class.

# Final Thoughts

Overall this was an interesting experience with building all of these models. The key takeaways I have learned are that the predictors may not be the same across all of the models. However, there will be a few predictors that appear commonly across all the models. We can take those predictors and use them to make inferences about what the biggest associations are with successful repayment.

Additionally techniques like weights help with improving model performance along with using some simpler models like Logistic Regression or Naive Bayes to get some insights.

However Class Imbalance still remains an issue that must be addressed through other techniques like under-sampling, etc. I will be experimenting more on these techniques with my group.
